[{"content":"Introduction\rIn lab 4, we will build a key/value storage system that \u0026ldquo;shards\u0026rdquo;, or partitions, the keys over a set of replica groups.\nA shard is a subset of the key/value pairs.\nFor example, all the keys starting with \u0026ldquo;a\u0026rdquo; might be one shard, all the keys starting with \u0026ldquo;b\u0026rdquo; another, etc. The reason for sharding is performance. Each replica group handles puts and gets for just a few of the shards, and the groups operate in parallel. thus total system throughput (puts and gets per unit time) increases in proportion to the number of groups.\nThe sharded key/value store has two components. First, the component is the \u0026quot;shard controller\u0026quot;. The shard controller decides which replica group should serve each shard. Second, a set of replica groups. Each replica group is responsible for a subset of the shards. A replica consists of a handful of servers that use Raft to replicate the group\u0026rsquo;s shards.\nClients consult the shard controller to find the replica group for a key, and replica groups consult the controller to find out what shards to serve. There is a single shard controller for the whole system, implemented as a fault-tolerant service using Raft.\nExperience Description\rTo get started lab4, read the experiment document:\nhttps://pdos.csail.mit.edu/6.824/labs/lab-shard.html\nIn 3A, we should implement the shard controller, in shardctrler/server.go and client.go. implementation must support the RPC interface described in shardctrler/common.go, which consists of Join, Leave, Move, and Query RPCs. These RPCs are intended to allow an administrator (and the tests) to control the shardctrler: to add new replica groups, to eliminate replica groups, and to move shards between replica groups.\nIn 3B, modify shardkv/client.go, shardkv/common.go, and shardkv/server.go.\nEach shardkv server operates as part of a replica group. Each replica group serves Get, Put, and Append operations for some of the key-space shards. Use key2shard() in client.go to find which shard a key belongs to. Multiple replica groups cooperate to serve the complete set of shards. A single instance of the shardctrler service assigns shards to replica groups; when this assignment changes, replica groups have to hand off shards to each other, while ensuring that clients do not see inconsistent responses.\nStorage system must provide a linearizable interface to applications that use its client interface. That is completed application calls to the Clerk.Get(), Clerk.Put(), and Clerk.Append() methods in shardkv/client.go must appear to have affected all replicas in the same order. A Clerk.Get() should see the value written by the most recent Put/Append to the same key. This must be true even when Gets and Puts arrive at about the same time as configuration changes.\nImplementation\rShard controller (4A)\rTo support Join, Leave, Move, and Query RPCs. Combine them into one command RPCs. Modify the common.go.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type CommandArgs struct { Servers map[int][]string //Join GIDs []int //leave Shard int //Move GID int //Move Num int //Query Op string // four optype ClientId int64 CommandId int } type CommandReply struct { Err Err Config Config } The Clerk and Server frame struct like Lab3, the controller using Raft the sync configuration log.\nThe main task is the implementation of four RPCs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 func (sc *ShardCtrler) JoinProcess(Servers map[int][]string) { lastConfig := sc.GetLastConfig() lastGroup := make(map[int][]string) // copy for k, v := range lastConfig.Groups { lastGroup[k] = v } newConfig := Config{ Num: lastConfig.Num + 1, Shards: lastConfig.Shards, Groups: lastGroup, } for gid, servers := range Servers { if _, ok := newConfig.Groups[gid]; !ok { newServers := make([]string, len(servers)) copy(newServers, servers) newConfig.Groups[gid] = newServers } } g2s := Group2Shard(newConfig) // find big and small untill blance for { from, to := getGid_MaxShards(g2s), getGid_MinShards(g2s) if from != 0 \u0026amp;\u0026amp; len(g2s[from])-len(g2s[to]) \u0026lt;= 1 { break } g2s[to] = append(g2s[to], g2s[from][0]) g2s[from] = g2s[from][1:] } var newShards [NShards]int for gid, shards := range g2s { for _, shard := range shards { newShards[shard] = gid } } newConfig.Shards = newShards sc.configs = append(sc.configs, newConfig) } func (sc *ShardCtrler) LeaveProcess(GIDs []int) { lastConfig := sc.GetLastConfig() lastGroup := make(map[int][]string) orphanShards := make([]int, 0) for k, v := range lastConfig.Groups { lastGroup[k] = v } newConfig := Config{Num: lastConfig.Num + 1, Shards: lastConfig.Shards, Groups: lastGroup} g2s := Group2Shard(newConfig) for _, gid := range GIDs { if _, ok := newConfig.Groups[gid]; ok { delete(newConfig.Groups, gid) } if shards, ok := g2s[gid]; ok { orphanShards = append(orphanShards, shards...) delete(g2s, gid) } } var newShards [NShards]int // find target small to add leave group if len(newConfig.Groups) != 0 { for _, shard := range orphanShards { to := getGid_MinShards(g2s) g2s[to] = append(g2s[to], shard) } for gid, shards := range g2s { for _, shard := range shards { newShards[shard] = gid } } } newConfig.Shards = newShards sc.configs = append(sc.configs, newConfig) } func (sc *ShardCtrler) MoveProcess(gid int, shard int) { lastConfig := sc.GetLastConfig() lastGroup := make(map[int][]string) for k, v := range lastConfig.Groups { lastGroup[k] = v } newConfig := Config{Num: lastConfig.Num + 1, Shards: lastConfig.Shards, Groups: lastGroup} newConfig.Shards[shard] = gid sc.configs = append(sc.configs, newConfig) } func (sc *ShardCtrler) QueryProcess(num int) Config { if num == -1 || num \u0026gt;= len(sc.configs) { return sc.configs[len(sc.configs)-1] } return sc.configs[num] } The apply() function and command() function to handle all logs and requests.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 func (sc *ShardCtrler) apply() { for !sc.killed() { select { case ch := \u0026lt;-sc.applyCh: if ch.CommandValid { sc.mu.Lock() opchan := sc.GetChan(ch.CommandIndex) op := ch.Command.(Op) res := result{ ClientId: op.ClientId, CommandId: op.CommandId, } if sc.Client2Seq[op.ClientId] \u0026lt; op.CommandId { switch op.Command { case Join: sc.JoinProcess(op.Servers) case Leave: sc.LeaveProcess(op.GIDs) case Move: sc.MoveProcess(op.GID, op.Shard) case Query: res.Res = sc.QueryProcess(op.Num) } sc.Client2Seq[op.ClientId] = op.CommandId sc.mu.Unlock() opchan \u0026lt;- res } else { sc.mu.Unlock() } } } } } func (sc *ShardCtrler) Command(args *CommandArgs, reply *CommandReply) { if sc.killed() { reply.Err = WrongLeader return } _, isLeader := sc.rf.GetState() if !isLeader { reply.Err = WrongLeader return } sc.mu.Lock() if sc.Client2Seq[args.ClientId] \u0026gt;= args.CommandId { reply.Err = OK if args.Op == Query { reply.Config = sc.QueryProcess(args.Num) } sc.mu.Unlock() return } op := Op{ Command: args.Op, CommandId: args.CommandId, ClientId: args.ClientId, Servers: args.Servers, //Join GIDs: args.GIDs, //leave Shard: args.Shard, //Move GID: args.GID, //Move Num: args.Num, //Query } index, _, isLeader := sc.rf.Start(op) if !isLeader { reply.Err = WrongLeader sc.mu.Unlock() return } ch := sc.GetChan(index) sc.mu.Unlock() select { case app := \u0026lt;-ch: if app.ClientId == op.ClientId \u0026amp;\u0026amp; app.CommandId == op.CommandId { reply.Err = OK reply.Config = app.Res } else { reply.Err = WrongLeader } case \u0026lt;-time.After(time.Millisecond * 33): reply.Err = TimeOut } go func() { sc.mu.Lock() delete(sc.Index2Cmd, index) sc.mu.Unlock() }() } Sharded Key/Value Server (4B)\rThe lab4 is the storage system, we should use raft(lab2) and shard controller(4A) to build a sharded key/value system. The total design is like lab3. We should modify it to fit the sharded environment, and make it approach a production environment.\nThere are two challenges:\nGarbage collection of state\nWhen a replica group loses ownership of a shard, that replica group should eliminate the keys that it lost from its database. It is wasteful for it to keep values that it no longer owns, and no longer serves requests for.\nClient requests during configuration changes\nThe simplest way to handle configuration changes is to disallow all client operations until the transition has been completed. While conceptually simple, this approach is not feasible in production-level systems; it results in long pauses for all clients whenever machines are brought in or taken out. It would be better to continue serving shards that are not affected by the ongoing configuration change.\nFirst, implement clerk and merge Get/Put/Append RPCS (like lab3) in common.go.\n1 2 3 4 5 6 7 8 9 10 11 12 type CommandArgs struct { Key string Value string Op Operation ClientId int64 CommandId int } type CommandReply struct { Value string Err Err } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 func (ck *Clerk) Get(key string) string { return ck.Command(\u0026amp;CommandArgs{Key: key, Op: GetType}) } func (ck *Clerk) PutAppend(key string, value string, op string) { ck.Command(\u0026amp;CommandArgs{Key: key, Value: value, Op: Operation(op)}) } func (ck *Clerk) Put(key string, value string) { ck.PutAppend(key, value, \u0026#34;Put\u0026#34;) } func (ck *Clerk) Append(key string, value string) { ck.PutAppend(key, value, \u0026#34;Append\u0026#34;) } func (ck *Clerk) Command(args *CommandArgs) string { args.ClientId = ck.ClientId ck.CommandId++ args.CommandId = ck.CommandId for { shardId := key2shard(args.Key) gid := ck.config.Shards[shardId] if servers, ok := ck.config.Groups[gid]; ok { for si := 0; si \u0026lt; len(servers); si++ { srv := ck.make_end(servers[si]) var reply CommandReply ok := srv.Call(\u0026#34;ShardKV.Command\u0026#34;, args, \u0026amp;reply) if ok \u0026amp;\u0026amp; (reply.Err == OK || reply.Err == ErrNoKey) { return reply.Value } if ok \u0026amp;\u0026amp; reply.Err == ErrWrongGroup { break } } } time.Sleep(100 * time.Millisecond) ck.config = ck.sm.Query(-1) } } ShardKV still refers the lab3, but we should add something and modify it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 type ShardKV struct { mu sync.Mutex me int rf *raft.Raft applyCh chan raft.ApplyMsg makeEnd func(string) *labrpc.ClientEnd gid int masters []*labrpc.ClientEnd maxRaftState int // snapshot if log grows this big // Your definitions here. dead int32 // set by Kill() Config shardctrler.Config // 需要更新的最新的配置 LastConfig shardctrler.Config // 更新之前的配置，用于比对是否全部更新完了 DB KVStateMachine // Storage system ComNotify map[int]chan OpReply // notify ClientId2ComId map[int64]int sck *shardctrler.Clerk // sck is a client used to contact shard master } func StartServer(servers []*labrpc.ClientEnd, me int, persister *raft.Persister, maxraftstate int, gid int, masters []*labrpc.ClientEnd, make_end func(string) *labrpc.ClientEnd) *ShardKV { // call labgob.Register on structures you want // Go\u0026#39;s RPC library to marshall/unmarshall. labgob.Register(Op{}) kv := new(ShardKV) kv.me = me kv.maxRaftState = maxraftstate kv.makeEnd = make_end kv.gid = gid kv.masters = masters // Your initialization code here. kv.DB = \u0026amp;KVMemory{make([]KVShard, shardctrler.NShards)} kv.ClientId2ComId = make(map[int64]int) // Use something like this to talk to the shardctrler: // kv.mck = shardctrler.MakeClerk(kv.masters) kv.sck = shardctrler.MakeClerk(kv.masters) kv.ComNotify = make(map[int]chan OpReply) snapshot := persister.ReadSnapshot() if len(snapshot) \u0026gt; 0 { kv.DecodeSnapShot(snapshot) } kv.applyCh = make(chan raft.ApplyMsg) kv.rf = raft.Make(servers, me, persister, kv.applyCh) go kv.apply() go kv.ConfigDetectedLoop() return kv } KVStateMachine to Describe DataBase, notice it is sharded.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 type KVStateMachine interface { Get(key string) (string, Err) Put(key, value string) Err Append(key, value string) Err GetShard(shardId int) *KVShard } // KVMemory implement KVStateMachine type KVMemory struct { KV []KVShard } type KVShard struct { KVS map[string]string ConfigNum int // what version this Shard is in } func (kv *KVMemory) Get(key string) (string, Err) { shardId := key2shard(key) value, ok := kv.KV[shardId].KVS[key] if ok { return value, OK } return \u0026#34;\u0026#34;, ErrNoKey } func (kv *KVMemory) Put(key, value string) Err { shardId := key2shard(key) kv.KV[shardId].KVS[key] = value return OK } func (kv *KVMemory) Append(key, value string) Err { shardId := key2shard(key) kv.KV[shardId].KVS[key] += value return OK } func (kv *KVMemory) GetShard(shardId int) *KVShard { return \u0026amp;kv.KV[shardId] } func (kv *ShardKV) applyStateMachine(op *Op) { switch op.OpType { case PutType: kv.DB.Put(op.Key, op.Value) case AppendType: kv.DB.Append(op.Key, op.Value) } } Then the ShardKV receives command RPCs, we need to handle it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 // Put Append Get func (kv *ShardKV) Command(args *CommandArgs, reply *CommandReply) { shardId := key2shard(args.Key) kv.mu.Lock() if kv.Config.Shards[shardId] != kv.gid { reply.Err = ErrWrongGroup kv.mu.Unlock() return } else if kv.DB.GetShard(shardId).KVS == nil { reply.Err = ShardNotArrived kv.mu.Unlock() return } kv.mu.Unlock() command := Op{ OpType: args.Op, Key: args.Key, Value: args.Value, ClientId: args.ClientId, CommandId: args.CommandId, ShardId: shardId, } reply.Err = kv.startCommand(command, TimeOut) // Get function should check twice if reply.Err == OK \u0026amp;\u0026amp; command.OpType == GetType { kv.mu.Lock() if kv.Config.Shards[shardId] != kv.gid { reply.Err = ErrWrongGroup } else if kv.DB.GetShard(shardId).KVS == nil { reply.Err = ShardNotArrived } else { reply.Value, reply.Err = kv.DB.Get(args.Key) } kv.mu.Unlock() } } // using raft to sync log func (kv *ShardKV) startCommand(command Op, timeoutPeriod time.Duration) Err { kv.mu.Lock() index, _, isLeader := kv.rf.Start(command) if !isLeader { kv.mu.Unlock() return ErrWrongLeader } ch := kv.getWaitCh(index) kv.mu.Unlock() timer := time.NewTicker(timeoutPeriod) defer timer.Stop() // wait to finish select { case re := \u0026lt;-ch: kv.mu.Lock() delete(kv.ComNotify, index) if re.CommandId != command.CommandId || re.ClientId != command.ClientId { // One way to do this is for the server to detect that it has lost leadership, // by noticing that a different request has appeared at the index returned by Start() kv.mu.Unlock() return ErrInconsistentData } kv.mu.Unlock() return re.Err case \u0026lt;-timer.C: return ErrOverTime } } The make should linearized and consistent, we should pull the latest configuration by shard controller.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 func (kv *ShardKV) ConfigDetectedLoop() { kv.mu.Lock() curConfig := kv.Config rf := kv.rf kv.mu.Unlock() for !kv.killed() { // only leader needs to deal with configuration tasks if _, isLeader := rf.GetState(); !isLeader { time.Sleep(UpConfigLoopInterval) continue } kv.mu.Lock() // send finished ? if !kv.allSent() { SeqMap := make(map[int64]int) for k, v := range kv.ClientId2ComId { SeqMap[k] = v } for shardId, gid := range kv.LastConfig.Shards { //check and send shard to other if gid == kv.gid \u0026amp;\u0026amp; kv.Config.Shards[shardId] != kv.gid \u0026amp;\u0026amp; kv.DB.GetShard(shardId).ConfigNum \u0026lt; kv.Config.Num { sendDate := kv.cloneShard(kv.Config.Num, kv.DB.GetShard(shardId).KVS) args := SendShardArg{ LastAppliedCommandId: SeqMap, ShardId: shardId, Shard: sendDate, ClientId: int64(gid), CommandId: kv.Config.Num, } // shardId -\u0026gt; gid -\u0026gt; server names serversList := kv.Config.Groups[kv.Config.Shards[shardId]] servers := make([]*labrpc.ClientEnd, len(serversList)) for i, name := range serversList { servers[i] = kv.makeEnd(name) } go kv.sendAddShrad(servers, \u0026amp;args) } } kv.mu.Unlock() time.Sleep(UpConfigLoopInterval) continue } // received finished ? if !kv.allReceived() { kv.mu.Unlock() time.Sleep(UpConfigLoopInterval) continue } // current configuration is configured, poll for the next configuration curConfig = kv.Config sck := kv.sck kv.mu.Unlock() // pull new configuration newConfig := sck.Query(curConfig.Num + 1) if newConfig.Num != curConfig.Num+1 { time.Sleep(UpConfigLoopInterval) continue } //sync pull op command := Op{ OpType: UpConfigType, ClientId: int64(kv.gid), CommandId: newConfig.Num, UpConfig: newConfig, } kv.startCommand(command, UpConfigTimeout) } } Send the shard from one shard to another shard, must using an RPCs, which is named AddShard.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 type SendShardArg struct { LastAppliedCommandId map[int64]int // for receiver to update its state ShardId int Shard KVShard // Shard to be sent ClientId int64 CommandId int } type AddShardReply struct { Err Err } func (kv *ShardKV) sendAddShrad(servers []*labrpc.ClientEnd, args *SendShardArg) { index := 0 start := time.Now() for { var reply AddShardReply ok := servers[index].Call(\u0026#34;ShardKV.AddShard\u0026#34;, args, \u0026amp;reply) // send shard success or timeout if ok \u0026amp;\u0026amp; reply.Err == OK || time.Now().Sub(start) \u0026gt;= 2*time.Second { kv.mu.Lock() command := Op{ OpType: RemoveShardType, ClientId: int64(kv.gid), ShardId: args.ShardId, CommandId: kv.Config.Num, } kv.mu.Unlock() // remove shard kv.startCommand(command, TimeOut) break } index = (index + 1) % len(servers) if index == 0 { time.Sleep(UpConfigLoopInterval) } } } func (kv *ShardKV) AddShard(args *SendShardArg, reply *AddShardReply) { command := Op{ OpType: AddShardType, ClientId: args.ClientId, CommandId: args.CommandId, ShardId: args.ShardId, Shard: args.Shard, ClientId2ComId: args.LastAppliedCommandId, } reply.Err = kv.startCommand(command, AddShardsTimeout) } At last, apply function to apply all op (Get/Append/Put, update config). The apply function is like lab3, but you should think about updating the config.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 func (kv *ShardKV) apply() { for { if kv.killed() { return } select { case ch := \u0026lt;-kv.applyCh: if ch.CommandValid == true { kv.mu.Lock() op := ch.Command.(Op) reply := OpReply{ ClientId: op.ClientId, CommandId: op.CommandId, Err: OK, } if op.OpType == PutType || op.OpType == GetType || op.OpType == AppendType { shardId := key2shard(op.Key) if kv.Config.Shards[shardId] != kv.gid { reply.Err = ErrWrongGroup } else if kv.DB.GetShard(shardId).KVS == nil { // 如果应该存在的切片没有数据那么这个切片就还没到达 reply.Err = ShardNotArrived } else { if !kv.ifDuplicate(op.ClientId, op.CommandId) { kv.ClientId2ComId[op.ClientId] = op.CommandId kv.applyStateMachine(\u0026amp;op) } } } else { // request from server of other group switch op.OpType { case UpConfigType: kv.upConfigHandler(op) case AddShardType: // 如果配置号比op的SeqId还低说明不是最新的配置 if kv.Config.Num \u0026lt; op.CommandId { reply.Err = ConfigNotArrived break } kv.addShardHandler(op) case RemoveShardType: // remove operation is from previous UpConfig kv.removeShardHandler(op) default: log.Fatalf(\u0026#34;invalid command type: %v.\u0026#34;, op.OpType) } } // 如果需要snapshot，且超出其stateSize if kv.maxRaftState != -1 \u0026amp;\u0026amp; kv.rf.GetRaftStateSize() \u0026gt; kv.maxRaftState { snapshot := kv.PersistSnapShot() kv.rf.Snapshot(ch.CommandIndex, snapshot) } ch := kv.getWaitCh(ch.CommandIndex) ch \u0026lt;- reply kv.mu.Unlock() } if ch.SnapshotValid == true { if len(ch.Snapshot) \u0026gt; 0 { // 读取快照的数据 kv.mu.Lock() kv.DecodeSnapShot(ch.Snapshot) kv.mu.Unlock() } continue } } } } The snapshot is very easy to implement, just refer lab3.\nReference\rThere are many difficulties in finishing lab4, there are blogs that may help you.\ngithub_lab4\ncdsn_lab4b\n","date":"2023-10-07T00:00:00Z","permalink":"http://localhost:1313/p/lab-4-sharded-key/value-service/","title":"Lab 4: Sharded Key/Value Service"},{"content":"Introduction\rIn lab3 we will build a fault-tolerant key/value storage service using your Raft library from Lab2. This service will be a replicated state machine, consisting of several key/value servers that use Raft for replication. This lab has two parts. In 3A, we will implement a key/value service using Raft, but without using snapshots. In 3B, we will use a snapshot from Lab2D, which allows Raft to discard old log entries.\nBefore starting the lab, you should review the extended Raft paper, in particular Sections 7 and 8.\nExperience Description\rTo get started lab3, read the experiment document:\nhttps://pdos.csail.mit.edu/6.824/labs/lab-kvraft.html\nClients can send three different RPCs to the key/value service: Put(key, value), Append(key, arg), and Get(key). Each client talks to the service through a Clerk with Put/Append/Get methods. A Clerk manages RPC interactions with the servers.\nAn important thing is Linearizability. Linearizability is convenient for applications because it\u0026rsquo;s the behavior you\u0026rsquo;d see from a single server that processes requests one at a time. It is harder if the service is replicated since all servers must choose the same execution order for concurrent requests, must avoid replying to clients using state that isn\u0026rsquo;t up to date, and must recover their state after a failure in a way that preserves all acknowledged client updates.\nAll code and tests are in src/kvraft. We will need to modify kvraft/client.go, kvraft/server.go, and perhaps kvraft/common.go.\nImplementation\rLinearizability\rTo make sure of the Linearizability, the paper gives a solution.\nFor example, if the leader crashes after committing the log entry but before responding to the client, the client will retry the command with a new leader, causing it to be executed a second time.\nThe solution is for clients to assign unique serial numbers to every command. Then the state machine tracks the latest serial number processed for each client, along with the associated response.\nIf it receives a command whose serial number has already been executed, it responds immediately without re-executing the request.\nClient (3A)\rTo make it easier to send RPCs, combine three different RPCs (Get/Put/Append) into one. So, we should modify the common.go.\n1 2 3 4 5 6 7 8 9 10 11 12 13 type CommandArgs struct { Op string // Get/Put/Append Key string // Get/Put/Append Value string // Put/Append CommandId int // linearizability ClientId int64 // linearizability } type CommandReply struct { Err Err // Put/Append Value string // Get } Then for Clerk client, we should make it send Command RPCS to each server which includes raft note. To assure Linearizability, we use ClientId and CommandId to mark a Clerk client.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 type Clerk struct { Leader Id int CommandId int ClientId int64 } func (ck *Clerk) Command(args *CommandArgs) string { args.ClientId = ck.ClientId args.CommandId = ck.CommandId LeaderId := ck.LeaderId for { reply := CommandReply{} ok := ck.servers[LeaderId].Call(\u0026#34;KVServer.Command\u0026#34;, args, \u0026amp;reply) if ok { switch reply.Err { case OK: ck.LeaderId = LeaderId ck.CommandId++ return reply.Value case ErrNoKey: ck.LeaderId = LeaderId ck.CommandId++ return \u0026#34;\u0026#34; } } LeaderId = (LeaderId + 1) % len(ck.servers) } } func (ck *Clerk) Get(key string) string { // You will have to modify this function. return ck.Command(\u0026amp;CommandArgs{Key: key, Value: \u0026#34;\u0026#34;, Op: \u0026#34;Get\u0026#34;}) } func (ck *Clerk) PutAppend(key string, value string, op string) { // You will have to modify this function. ck.Command(\u0026amp;CommandArgs{Key: key, Value: value, Op: op}) } func (ck *Clerk) Put(key string, value string) { ck.PutAppend(key, value, \u0026#34;Put\u0026#34;) } func (ck *Clerk) Append(key string, value string) { ck.PutAppend(key, value, \u0026#34;Append\u0026#34;) } Server (3A)\rAs the paper, the state machine should track the latest serial number processed for each client, along with the associated response. So using Client2ComId map[int64]int and ComNotify map[int]chan Op record them.\n1 2 3 4 5 6 7 type KVServer struct { LastApplied int StateMachine KVStateMachine Client2ComId map[int64]int ComNotify map[int] chan Op } Using KVStateMachine to Describe the kv storage structure (like a database).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 type KVStateMachine interface { Get(key string) (string, Err) Put(key, value string) Err Append(key, value string) Err } // kv datebase implement kvstatemachine type MemoryKV struct { KV map[string]string } func (kv *MemoryKV) Get(key string) (string, Err) { value, ok := kv.KV[key] if ok { return value, OK } return \u0026#34;\u0026#34;, ErrNoKey } func (kv *MemoryKV) Put(key, value string) Err { kv.KV[key] = value return OK } func (kv *MemoryKV) Append(key, value string) Err { kv.KV[key] += value return OK } When the server receives an RPC from the Client, it should sync it by Raft library, then wait until the raft library sends a message by channel. The command method used to sync by raft, the apply function listens to channel message and apply to KVStateMachine.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 func (kv *KVServer) Command(args *CommandArgs, reply *CommandReply) { if kv.killed() { reply.Err = ErrWrongLeader return } kv.mu.Lock() // put and append re-executing if args.Op != \u0026#34;Get\u0026#34; \u0026amp;\u0026amp; kv.ClientId2ComId[args.ClientId] \u0026gt;= args.CommandId { reply.Err = OK kv.mu.Unlock() return } kv.mu.Unlock() op := Op{ Key: args.Key, Value: args.Value, Command: args.Op, CommandId: args.CommandId, ClientId: args.ClientId, } // using raft library to sync log index, _, isLeader := kv.rf.Start(op) if !isLeader { reply.Err = ErrWrongLeader return } kv.mu.Lock() ch := kv.GetChan(index) kv.mu.Unlock() // waiting message from raft select { case apply := \u0026lt;-ch: if apply.ClientId == op.ClientId \u0026amp;\u0026amp; apply.CommandId == op.CommandId { // Get if args.Op == \u0026#34;Get\u0026#34; { kv.mu.Lock() reply.Value, reply.Err = kv.StateMachine.Get(apply.Key) kv.mu.Unlock() } // Put or Append reply.Err = OK } else { // timeout, it should re-executing reply.Err = TimeOut } case \u0026lt;-time.After(time.Millisecond * 33): reply.Err = TimeOut } // delete channel go func() { kv.mu.Lock() delete(kv.ComNotify, index) kv.mu.Unlock() }() } func (kv *KVServer) apply() { for !kv.killed() { select { // server get applych from raftnote case ch := \u0026lt;-kv.applyCh: // command sycn success if ch.CommandValid { kv.mu.Lock() if ch.CommandIndex \u0026lt;= kv.LastApplied { kv.mu.Unlock() continue } kv.LastApplied = ch.CommandIndex opchan := kv.GetChan(ch.CommandIndex) op := ch.Command.(Op) // apply to stateMachine(kvdatebase) if kv.ClientId2ComId[op.ClientId] \u0026lt; op.CommandId { kv.applyStateMachine(\u0026amp;op) kv.ClientId2ComId[op.ClientId] = op.CommandId } kv.mu.Unlock() opchan \u0026lt;- op } } } } SnapShot (3B)\rThe snapshot must include the KVStateMachine (database). The ClientLd2ComId also should be stored, which can avoid command execution a second time.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 func (kv *KVServer) apply() { for !kv.killed() { select { // server get applych from raftnote case ch := \u0026lt;-kv.applyCh: // an apply if ch.CommandValid { ..... if kv.maxraftstate != -1 \u0026amp;\u0026amp; kv.rf.GetRaftStateSize() \u0026gt; kv.maxraftstate { kv.rf.Snapshot(ch.CommandIndex, kv.PersisterSnapshot()) } ..... } // a snap if ch.SnapshotValid { kv.mu.Lock() if ch.SnapshotIndex \u0026gt; kv.LastApplied { kv.DecodeSnapshot(ch.Snapshot) kv.LastApplied = ch.SnapshotIndex } kv.mu.Unlock() } } } } // start func StartKVServer(servers []*labrpc.ClientEnd, me int, persister *raft.Persister, maxraftstate int) *KVServer { // call labgob.Register on structures you want // Go\u0026#39;s RPC library to marshall/unmarshall. labgob.Register(Op{}) kv := new(KVServer) kv.me = me kv.maxraftstate = maxraftstate // You may need initialization code here. kv.applyCh = make(chan raft.ApplyMsg) kv.rf = raft.Make(servers, me, persister, kv.applyCh) kv.ClientId2ComId = make(map[int64]int) kv.ComNotify = make(map[int]chan Op) kv.StateMachine = \u0026amp;MemoryKV{make(map[string]string)} // read snapshot snapshot := persister.ReadSnapshot() if len(snapshot) \u0026gt; 0 { kv.DecodeSnapshot(snapshot) } go kv.apply() return kv } ","date":"2023-09-30T00:00:00Z","permalink":"http://localhost:1313/p/lab-3-fault-tolerant-key/value-service/","title":"Lab 3: Fault-tolerant Key/Value Service"},{"content":"Introduction\rIn lab2 we will implement Raft, a replicated state machine protocol. This lab is divided into four parts, 2A should finish the leader election, 2B must make sure to sync the log between each server, 2C will persist in every server state and 2D take log compaction to reduce logs.\nAfter finishing lab1, we became familiar with Golang, but the concurrency programming is still hard. There are many details to pay attention to Raft, so read the Raft Paper carefully and repeatedly.\nExperiment Description\rTo get started lab2, read the experiment document:\nhttps://pdos.csail.mit.edu/6.824/labs/lab-raft.html\nAll code should be implemented in src/raft/raft.go. The tests are in src/raft/test_test.go\nImplementation\rLeader election(2A)\rIn 2A, we should implement leader election and heartbeats(empty AppendEntries RPCs). As described in the paper, Fill Some State in Raft struct:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 type Raft stuct { mu sync.Mutex // Lock to protect shared access to this peer\u0026#39;s state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer\u0026#39;s persisted state me int // this peer\u0026#39;s index into peers[] dead int32 // set by Kill() // Your data here (2A, 2B, 2C). // Look at the paper\u0026#39;s Figure 2 for a description of what // state a Raft server must maintain. // 2A Time time.Time // check leader elect timeout state int // server state currentTerm int // term votedFor int // voted to which server voteCount int // receive vote number Logs []Log // log entry // 2B commitIndex int // commit lastApplied int nextIndex []int // nextIndex send to follower matchIndex []int // follower match Index applyCh chan ApplyMsg // send to service command apply // 2D Lastindex int // snap last index LastTerm int // snap last term } type Log stuct { Command interface{} Term int } // server state const ( Follower = iota Candidate Leader ) Then the server will check leader whether is alive periodically. But the timeout must be random because if two servers become candidates at the same and they still have the same timeout, the cluster may never elect a leader. If a candidate wins the election, it will broadcast to other servers, which will become followers. If don\u0026rsquo;t have the leader exceed the timeout period, the new election will start.\nIn the lab document, it was noticed don\u0026rsquo;t use Go\u0026rsquo;s time.Timer or time.Ticker.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // check state func (rf *Raft) ticker() { for !rf.killed() { // Your code here (2A) // Check if a leader election should be started. rf.mu.Lock() switch rf.state { case Follower: if time.Since(rf.Time) \u0026gt; randomTimeout() { go rf.StartElection() } case Candidate: if time.Since(rf.Time) \u0026gt; randomTimeout() { go rf.StartElection() } case Leader: rf.Broadcast() } rf.mu.Unlock() // pause for a random amount of time between 50 and 350 // milliseconds. ms := 30 + (rand.Int63() % 30) time.Sleep(time.Duration(ms) * time.Millisecond) } } When a follower starts to election, it will become a candidate and increment its term. And it will send RequestVote RPCs to another server. When the request is received, it will vote according to the rules(In Raft paper).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 func (rf *Raft) StartElection() { rf.mu.Lock() rf.state = Candidate rf.currentTerm += 1 // vote to itself rf.voteCount = 1 rf.votedFor = rf.me rf.persist() args := RequestVoteArgs{ Term: rf.currentTerm, CandidateId: rf.me, LastLogIndex: rf.getLastIndex(), LastLogTerm: rf.getLastLogTerm(), } rf.mu.Unlock() for i := range rf.peers { if i != rf.me { go rf.sendRequestVote(i, \u0026amp;args, \u0026amp;RequestVoteReply{}) } } } func (rf *Raft) sendRequestVote(server int, args *RequestVoteArgs, reply *RequestVoteReply) { ok := rf.peers[server].Call(\u0026#34;Raft.RequestVote\u0026#34;, args, reply) rf.mu.Lock() defer rf.mu.Unlock() // vote rule if !ok || rf.state != Candidate || reply.Term != rf.currentTerm { return } if reply.VoteGranted { rf.voteCount++ // exceed half win the election if rf.voteCount \u0026gt; len(rf.peers)/2 { rf.SetLeader() } } else { // vote rule if reply.Term \u0026gt; rf.currentTerm { rf.state = Follower rf.currentTerm = reply.Term rf.votedFor = -1 rf.Time = time.Now() rf.persist() } } } RequestVote RPCs\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 type RequestVoteArgs struct { // Your data here (2A, 2B). Term int CandidateId int LastLogIndex int LastLogTerm int } type RequestVoteReply struct { // Your data here (2A). Term int VoteGranted bool } func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { // Your code here (2A, 2B). rf.mu.Lock() defer rf.mu.Unlock() // overdue vote if args.Term \u0026lt; rf.currentTerm { reply.VoteGranted = false reply.Term = rf.currentTerm return } // log unqualified if args.LastLogTerm \u0026lt; rf.getLastLogTerm() || (args.LastLogTerm == rf.getLastLogTerm() \u0026amp;\u0026amp; args.LastLogIndex \u0026lt; rf.getLastIndex()) { reply.VoteGranted = false reply.Term = rf.currentTerm if args.Term \u0026gt; rf.currentTerm { rf.currentTerm = args.Term rf.state = Follower rf.persist() } return } // receive a vote should stay follower and vote if args.Term \u0026gt; rf.currentTerm || (args.Term == rf.currentTerm \u0026amp;\u0026amp; (rf.votedFor == -1 || rf.votedFor == args.CandidateId)) { rf.state = Follower rf.votedFor = args.CandidateId rf.currentTerm = args.Term reply.VoteGranted = true reply.Term = rf.currentTerm rf.Time = time.Now() rf.persist() } } Log Replication(2B)\rIn 2B, complete the Start() function (when a service sends a new command, it will sync Log)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 func (rf *Raft) Start(command interface{}) (int, int, bool) { index := -1 term := -1 isLeader := true // Your code here (2B). rf.mu.Lock() defer rf.mu.Unlock() term = rf.currentTerm if rf.state != Leader { return index, term, false } newLog := Log{ Command: command, Term: rf.currentTerm, } rf.logs = append(rf.logs, newLog) rf.persist() rf.matchIndex[rf.me] = len(rf.logs) - 1 + rf.Lastindex rf.nextIndex[rf.me] = rf.matchIndex[rf.me] + 1 for i := range rf.peers { if i == rf.me { continue } if rf.matchIndex[i] \u0026lt; rf.Lastindex { //do nothing } else { entry := make([]Log, rf.getLastIndex()-rf.matchIndex[i]) copy(entry, rf.logs[rf.matchIndex[i]+1-rf.Lastindex:]) // sync Log nargs := AppendEntriesArgs{ Term: rf.currentTerm, LeaderId: rf.me, PrevLogIndex: rf.matchIndex[i], PrevLogTerm: rf.getLogTerm(rf.matchIndex[i]), LeaderCommit: rf.commitIndex, Entries: entry, } go rf.sendAppenEntries(i, \u0026amp;nargs, \u0026amp;AppendEntriesReply{}) } } return len(rf.logs) - 1 + rf.Lastindex, newLog.Term, isLeader } When SendAppendEntries RPCs, the follower log may conflict with the Leader log, in which case it must resend the RPC. Sometimes followers may crash and restart or maintain follower status, the leader should synchronize logs or heartbeats via boardcast.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 func (rf *Raft) sendAppenEntries(server int, args *AppendEntriesArgs, reply *AppendEntriesReply) { ok := rf.peers[server].Call(\u0026#34;Raft.AppendEntries\u0026#34;, args, reply) if !ok { return } //log.Println(\u0026#34;send hb\u0026#34;) rf.mu.Lock() defer rf.mu.Unlock() if rf.state != Leader { return } if !reply.Success { if reply.Term \u0026gt; rf.currentTerm { rf.currentTerm = reply.Term rf.state = Follower rf.votedFor = -1 rf.persist() } else { args.PrevLogIndex = reply.Index if args.PrevLogIndex \u0026lt; 0 { return } if args.PrevLogIndex-rf.Lastindex \u0026lt; 0 { // send snap (2D) } else { // retry args.PrevLogTerm = rf.getLogTerm(args.PrevLogIndex) entry := make([]Log, rf.getLastIndex()-args.PrevLogIndex) copy(entry, rf.logs[args.PrevLogIndex-rf.Lastindex+1:]) args.Entries = entry // go syncLog go rf.sendAppenEntries(server, args, reply) } } } else { // sync log success if rf.matchIndex[server] \u0026lt; args.PrevLogIndex+len(args.Entries) { rf.matchIndex[server] = args.PrevLogIndex + len(args.Entries) // commit log rf.UpdateCommit() } if rf.nextIndex[server] \u0026lt; args.PrevLogIndex+len(args.Entries)+1 { rf.nextIndex[server] = args.PrevLogIndex + len(args.Entries) + 1 } } } Broadcast to each follower\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 func (rf *Raft) Broadcast() { if rf.state != Leader { return } prelogindex := rf.getLastIndex() prelogterm := rf.getLastLogTerm() rf.UpdateCommit() for i := range rf.peers { if i == rf.me { continue } // log not match if (rf.nextIndex[i] \u0026lt;= prelogindex || rf.nextIndex[i]-rf.matchIndex[i] != 1) \u0026amp;\u0026amp; rf.getLastIndex() != 0 { if rf.matchIndex[i] \u0026lt; rf.Lastindex { // need log is remove // send snapshot (2D) } else { // send synclog entry := make([]Log, rf.getLastIndex()-rf.matchIndex[i]) copy(entry, rf.logs[rf.matchIndex[i]+1-rf.Lastindex:]) nargs := AppendEntriesArgs{ Term: rf.currentTerm, LeaderId: rf.me, PrevLogIndex: rf.matchIndex[i], PrevLogTerm: rf.getLogTerm(rf.matchIndex[i]), LeaderCommit: rf.commitIndex, Entries: entry, } go rf.sendAppenEntries(i, \u0026amp;nargs, \u0026amp;AppendEntriesReply{}) } } else { // log match send heartbeat args := AppendEntriesArgs{ Term: rf.currentTerm, LeaderId: rf.me, PrevLogIndex: prelogindex, PrevLogTerm: prelogterm, LeaderCommit: rf.commitIndex, } go rf.sendAppenEntries(i, \u0026amp;args, \u0026amp;AppendEntriesReply{}) } } } AppendEntries RPCs will keep the server aligned with the leader. There\u0026rsquo;s a lot of detail in the code. It\u0026rsquo;s too long and too big.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 type AppendEntriesArgs struct { Term int LeaderId int PrevLogIndex int PrevLogTerm int LeaderCommit int Entries []Log } type AppendEntriesReply struct { Term int Success bool Index int } func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { rf.mu.Lock() defer rf.mu.Unlock() // term not match reply.Success = false if args.Term \u0026lt; rf.currentTerm { reply.Term = rf.currentTerm return } else if args.Term \u0026gt; rf.currentTerm { reply.Term = args.Term rf.currentTerm = args.Term rf.Time = time.Now() rf.state = Follower rf.votedFor = -1 rf.persist() } else { //term equal rf.Time = time.Now() rf.state = Follower reply.Term = args.Term } //lack some logs if rf.getLastIndex() \u0026lt; args.PrevLogIndex { reply.Index = rf.getLastIndex() return } if rf.Lastindex \u0026gt; args.PrevLogIndex { if args.PrevLogIndex+len(args.Entries) \u0026lt;= rf.Lastindex { reply.Index = rf.Lastindex return } args.PrevLogTerm = args.Entries[rf.Lastindex-args.PrevLogIndex-1].Term args.Entries = args.Entries[rf.Lastindex-args.PrevLogIndex:] args.PrevLogIndex = rf.Lastindex } if args.PrevLogTerm != rf.getLogTerm(args.PrevLogIndex) { reply.Index = rf.lastApplied if reply.Index \u0026gt; rf.Lastindex { reply.Index = rf.Lastindex } if reply.Index \u0026gt; args.PrevLogIndex-1 { reply.Index = args.PrevLogIndex - 1 } return } // sync sucess reply.Success = true //latest condition if rf.getLastIndex() == args.PrevLogIndex \u0026amp;\u0026amp; args.PrevLogTerm == rf.getLastLogTerm() { if args.LeaderCommit \u0026gt; rf.commitIndex { tmp := rf.getLastIndex() if tmp \u0026gt; args.LeaderCommit { tmp = args.LeaderCommit } rf.commitIndex = tmp } } //heart beat if len(args.Entries) == 0 { return } // overdue entries if rf.getLastIndex() \u0026gt;= args.PrevLogIndex+len(args.Entries) \u0026amp;\u0026amp; rf.getLogTerm(args.PrevLogIndex+len(args.Entries)) == args.Entries[len(args.Entries)-1].Term { return } i := args.PrevLogIndex + 1 for i \u0026lt;= rf.getLastIndex() \u0026amp;\u0026amp; i-args.PrevLogIndex-1 \u0026lt; len(args.Entries) { break } if i-args.PrevLogIndex-1 \u0026gt;= len(args.Entries) { return } // append to itself rf.logs = rf.logs[:i-rf.Lastindex] rf.logs = append(rf.logs, args.Entries[i-args.PrevLogIndex-1:]...) // commit and persist if args.LeaderCommit \u0026gt; rf.commitIndex { tmp := rf.getLastIndex() if tmp \u0026gt; args.LeaderCommit { tmp = args.LeaderCommit } rf.commitIndex = tmp } rf.persist() } Using loops, detect if there are submitted but not applied entries and send the entry information to the channel provided by the test system.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 unc (rf *Raft) apply() { for !rf.killed() { rf.mu.Lock() oldApply := rf.lastApplied oldCommit := rf.commitIndex //after crash if oldApply \u0026lt; rf.Lastindex { rf.lastApplied = rf.Lastindex rf.commitIndex = rf.Lastindex rf.mu.Unlock() time.Sleep(time.Millisecond * 30) continue } if oldCommit \u0026lt; rf.Lastindex { rf.commitIndex = rf.Lastindex rf.mu.Unlock() time.Sleep(time.Millisecond * 30) continue } if oldApply == oldCommit || (oldCommit-oldApply) \u0026gt;= len(rf.logs) { rf.mu.Unlock() time.Sleep(time.Millisecond * 5) continue } entry := make([]Log, oldCommit-oldApply) copy(entry, rf.logs[oldApply+1-rf.Lastindex:oldCommit+1-rf.Lastindex]) rf.mu.Unlock() // apply for key, value := range entry { rf.applyCh \u0026lt;- ApplyMsg{ CommandValid: true, CommandIndex: key + oldApply + 1, Command: value.Command, } } rf.mu.Lock() if rf.lastApplied \u0026lt; oldCommit { rf.lastApplied = oldCommit } if rf.lastApplied \u0026gt; rf.commitIndex { rf.commitIndex = rf.lastApplied } rf.mu.Unlock() time.Sleep(time.Millisecond * 30) } } Persistence (2C)\rThis is as simple as storing persistent state on a server (described in the Raft paper).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 func (rf *Raft) getRaftState() []byte { w := new(bytes.Buffer) e := labgob.NewEncoder(w) e.Encode(rf.votedFor) e.Encode(rf.currentTerm) e.Encode(rf.logs) e.Encode(rf.Lastindex) e.Encode(rf.LastTerm) return w.Bytes() } // persist in local func (rf *Raft) persist() { rf.persister.Save(rf.getRaftState(), rf.persister.snapshot) } // read from local func (rf *Raft) readPersist(data []byte) { if data == nil || len(data) \u0026lt; 1 { return } // Your code here (2C). r := bytes.NewBuffer(data) d := labgob.NewDecoder(r) var votefor int //Votefor var currentTerm int //term var logs []Log //Logs var index int //index var term int //term rf.mu.Lock() defer rf.mu.Unlock() if d.Decode(\u0026amp;votefor) != nil || d.Decode(\u0026amp;currentTerm) != nil || d.Decode(\u0026amp;logs) != nil || d.Decode(\u0026amp;index) != nil || d.Decode(\u0026amp;term) != nil { } else { rf.votedFor = votefor rf.currentTerm = currentTerm rf.logs = logs rf.Lastindex = index rf.LastTerm = term } } Log Compaction (2D)\rRead the paper carefully to understand how a SnapShot can create a log compaction that compacts unrestricted log growth and reduces the storage pressure on peers.\nAfter SnapShot is imported, the original Index system will be greatly changed. The changes related to Index log information need to consider lastSnapShotIndex\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func (rf *Raft) Snapshot(index int, snapshot []byte) { // Your code here (2D). rf.mu.Lock() defer rf.mu.Unlock() if index \u0026lt;= rf.Lastindex || index \u0026gt; rf.commitIndex { return } // Clipping log count := 1 oldIndex := rf.Lastindex for offset, value := range rf.logs { if offset == 0 { continue } count++ rf.Lastindex = offset + oldIndex rf.LastTerm = value.Term if offset+oldIndex == index { break } } newLog := make([]Log, 1) newLog = append(newLog, rf.logs[count:]...) rf.logs = newLog rf.persister.Save(rf.getRaftState(), snapshot) } InstallSnapRPC\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 type InstallSnapshotRPC struct { Term int LeaderId int LastIncludeIndex int LastIncludeTerm int //offset int Data []byte } type InstallSnapshotReply struct { Term int } func (rf *Raft) InstallSnapShot(args *InstallSnapshotRPC, reply *InstallSnapshotReply) { rf.mu.Lock() defer rf.mu.Unlock() reply.Term = rf.currentTerm if args.Term \u0026lt; rf.currentTerm { return } if args.Term \u0026gt; rf.currentTerm { rf.currentTerm = args.Term rf.votedFor = -1 rf.state = Follower rf.persist() } if args.LastIncludeIndex \u0026lt;= rf.Lastindex { return } rf.Time = time.Now() // remove old log tmpLog := make([]Log, 1) if rf.getLastIndex() \u0026gt; args.LastIncludeIndex+1 { tmpLog = append(tmpLog, rf.logs[args.LastIncludeIndex+1-rf.Lastindex:]...) } rf.Lastindex = args.LastIncludeIndex rf.LastTerm = args.LastIncludeTerm rf.logs = tmpLog if args.LastIncludeIndex \u0026gt; rf.commitIndex { rf.commitIndex = args.LastIncludeIndex } if args.LastIncludeIndex \u0026gt; rf.lastApplied { rf.lastApplied = args.LastIncludeIndex } rf.persister.Save(rf.getRaftState(), args.Data) msg := ApplyMsg{ Snapshot: args.Data, SnapshotValid: true, SnapshotTerm: rf.LastTerm, SnapshotIndex: rf.Lastindex, } go func() { rf.applyCh \u0026lt;- msg }() } ","date":"2023-09-05T00:00:00Z","permalink":"http://localhost:1313/p/lab-2-raft/","title":"Lab 2: Raft"},{"content":"Introduction\rRaft is a consensus algorithm for managing a replicated log. Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members. Although Paxos can solve consensus, it is indigestible for most people. Raft enhance understandability and also provides a better foundation for building practical system.\nThis paper called In Search of an Understandable Consensus Algorithm(Extended Version) was published in 2014 by Stanford University. This paper is very in-depth, I recommend reading it several times.\nReplicated state machines\rConsensus algorithms typically arise in the context of replicated state machines. Replicated state machines are typically implemented using a replicated log. The consensus algorithms manage a replicated log containing state machine commands from clients. The state machines process identical sequences of commands from the logs so that they produce the same outputs.\nThe Raft consensus algorithm\rRaft decomposes the consensus problem into three relatively independent subproblems:\nLeader election: A new leader must be chosen when an existing leader fails\nLog replication: The leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with their own.\nSafety: If any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index.\nRaft basics\rAt any time Raft cluster, each server is in one of three states: leader, follower, or candidate. In normal operation there is exactly one leader and all of the other servers are followers.\nFollowers are passive: They issue no requests on their own but simply respond to requests from leaders and candidates. The leader handles all client requests. The candidate elects a new leader.\nRaft divides time into terms of arbitrary length. Each term begins with an election, in which one or more candidates attempt to become leader. If a candidate wins the election, then it serves as a leader and other servers become followers.\nEach server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate; if one server’s current term is smaller than the other’s, then it updates its current term to the larger value. If a candidate or leader discovers that its term is out of date, it immediately reverts to follower state. If a server receives a request with a stale term number, it rejects the request.\nRaft servers communicate using remote procedure calls, and the basic consensus algorithm requires only two types of RPCs. RequestVote RPCs are initiated by candidates during elections, and AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat.\nLeader election\rA server remains in follower state as long as it receives valid RPCs from a leader or candidate. Leaders send periodic heartbeats to all followers to maintain their authority. If a follower receives no communication over a period called the election timeout, then it assumes there is no viable leader and begins an election to choose a new leader.\nTo begin an election, a follower increments its current term and transitions to candidate state. It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster. A candidate continues in this state until one of three things happens:\n(a) it wins the election\n(b) another server establishes itself as a leader\n(c) a period goes by with no winner.\nRaft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly. To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150–300ms).\nLog replication\rEach client request contains a command to be executed by the replicated state machines. The leader appends the command to its log as a new entry, then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry. When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client.\nEach log entry stores a state machine command along with the term number when the entry was received by the leader. Each log entry also has an integer index identifying its position in the log.\nRaft guarantees that committed entries are durable and will eventually be executed by all of the available state machines. A log entry is committed once the leader that created the entry has replicated it on a majority of the servers. This also commits all preceding entries in the leader’s log, including entries created by previous leaders. The leader keeps track of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs (including heartbeats) so that the other servers eventually find out. Once a follower learns that a log entry is committed, it applies the entry to its local state machine (in log order).\nRaft maintains the following properties, which together constitute the Log Matching Property:\nIf two entries in different logs have the same index and term, then they store the same command.\nIf two entries in different logs have the same index and term, then the logs are identical in all preceding entries.\nWhen sending an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries. If the follower does not find an entry in its log with the same index and term, then it refuses the new entries.\nTo bring a follower’s log into consistency with its own, the leader must find the latest log entry where the two logs agree, delete any entries in the follower’s log after that point, and send the follower all of the leader’s entries after that point. All of these actions happen in response to the consistency check performed by AppendEntries RPCs. The leader maintains a nextIndex for each follower, which is the index of the next log entry the leader will send to that follower. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log. If a follower’s log is inconsistent with the leader’s, the AppendEntries consistency check will fail in the next AppendEntries RPC. After a rejection, the leader decrements nextIndex and retries the AppendEntries RPC.\nSafety\rThe previous sections described how Raft elects leaders and replicates log entries. However, the mechanisms described so far are not quite sufficient to ensure that each state machine executes the same commands in the same order. For example, a follower might be unavailable while the leader commits several log entries, then it could be elected leader and overwrite these entries with new ones; as a result, different state machines might execute different command sequences.\nElection restriction\rRaft uses the voting process to prevent a candidate from winning an election unless its log contains all committed entries. A candidate must contact a majority of the cluster to be elected, which means that every committed entry must be present in at least one of those servers. If the candidate’s log is at least as up-to-date as any other log in that majority (where “up-to-date” is defined precisely below), then it will hold all the committed entries.\nCommiting entries from previous terms\rOnce a log entry for the current term is accepted by the majority of machines(half), the Leader commits it. If the Leader dies while submitting the entry, the next Leader will continue to try to complete the copy of the entry. However, the Leader cannot immediately confirm that an entry from a previous term has been committed, even if it is already stored on most machines.\nTherefore, Raft does not attempt to submit log entries from the previous term by checking the number of copies. Only the number of copies is checked to submit a log entry for the current term. When an entry for the current term is submitted, all entries currently in that entry are implicitly submitted.\nCluster membership changes\rFor the configuration change mechanism to be safe, there must be no point during the transition where two leaders can be elected for the same term.\nTo ensure the security of node changes, Raft adopts a two-stage approach. First, the cluster switches to a joint consensus state, and after the joint consensus is committed, the system switches to the new configuration.\nLog entries are replicated to all servers in both configurations.\nAny server from either configuration may serve as a leader.\nThe agreement requires separate majorities from both the old and new configurations.\nThe cluster configuration uses special entry storage and transport in the log. The process is as follows:\nThe Leader receives a request and switches the cluster configuration from C_old to C_new\nThe Leader stores C_old and C_new as the joint consensus C_old, new configuration in a log entry\nAppend this log entry to all machines with the old and new configurations.\nWhen a machine receives such a log entry and adds it to its logs (no submission is required), all subsequent operations are performed based on this configuration.\nWhen C_old, new is accepted by most machines, the Leader commits it. In this case, any machine configured with C_old or C_new cannot be selected as the Leader.\nThe Leader creates a log entry for C_new, appends it to all machines, and submits it\nThe following problems still exist when the cluster member changes:\nThe new machine does not store any logs when it joins the cluster, and it takes a while to catch up with the old machine, which may reduce the availability of the cluster in a short period. When adding nodes to a cluster, Raft introduces a new phase in which new machines can normally receive additional requests, but are not considered voting nodes, and consensus can be reached without considering these new nodes. Perform the preceding operations when the logs of the new node catch up with the progress of the old node.\nThe Leader of the cluster may not be part of the new configuration. In this case, when the Leader submits C_new, it should itself be pulled down. This results in a period when the Leader manages a cluster that does not contain itself and replicates logs but does not consider itself a principal.\nThose servers that are taken down may break the cluster, and since these machines cannot receive heartbeats, elections may be held. A canvassing request with a new term is then sent, causing the cluster Leader to revert to the Follower. Although the election will not succeed in this case, the new Leader will still be generated in the new cluster, but the removed machine will still time out the election, resulting in poor overall availability.\nTo prevent problem 3 from happening, Raft adds a restriction: If the server receives a vote request within the timeout period of receiving a heartbeat from the current Leader, it will not renew the term and vote. This allows a Leader not to be ousted by a larger term vote as long as he can maintain the heartbeat of the current cluster.\nLog compaction\rAs logs grow, the machine cannot store all logs in memory, so snapshots need to be introduced to periodically keep the state of the system in persistent storage so that logs from the start to the snapshot point are safely removed from memory.\nEach machine in the cluster independently manages its snapshot, which contains only the committed log entries in its log. In addition to saving the current state of the state machine, you also need to save some metadata:\nLog index of the last log entry included in the snapshot\nLog term of the last log entry included in the snapshot\nThis metadata is mainly used for continuity checks on append requests (where the previous log entries need to be compared). If you want to support the cluster node changes mentioned above, the snapshot must also contain the latest configuration information at the snapshot point. After the snapshot is written, all logs and historical snapshots before the snapshot point are deleted.\nOccasionally, when the Leader needs to synchronize its logs to some newly added or backward nodes, it needs to send snapshots. The leader will send installSnapshotRPC:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type SnapshotRequest struct { term // leader term leaderId // so follower can redirect clients lastIncludedIndex // the snapshot replaces all entries up through and including this inedex lastIncludedTerm // term of lastIncludeIndex offset // byte offset where chunk is positioned in the snapshot file data[] // raw bytes of the snapshot chunk, staring at offset done // true if this is the last chunk } type SnapshotReply struct { term // currentTerm, for leader to update itself } Reply immediately if term \u0026lt; currentTerm\nCreate new snapshot file if first chunk (offset is 0)\nWrite data into snapshot file at given offset\nReply and wait for more data chunks if done is false\nSave snapshot file, discard any existing or partial snapshot with a smaller index\nIf existing log entry has same index and term as snapshot’s last included entry, retain log entries following it and reply\nDiscard the entire log\nReset state machine using snapshot contents (and load snapshot’s cluster configuration)\nRaft Implementation\rserver\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type Raftnode struct { // persisten state on all servers currentTerm // latest term server has seen (initialized to 0 on first boot, increases monotonically) votedFor // candidateId that received vote in current term (or null if none) log[] // log entries; each entry contains command for state machine, and term when entry was received by leader (first index is 1) // volatile state on all servers commitIndex // index of highest log entry known to be committed (initialized to 0, increases monotonically) lastApplied // index of highest log entry applied to state machine (initialized to 0, increases monotonically) // volatile state on leaders nextIndex[] // for each server, index of the next log entry to send to that server (initialized to leader last log index + 1) matchIndex[] // for each server, index of highest log entry known to be replicated on server (initialized to 0, increases monotonically) } AppenEntries\n1 2 3 4 5 6 7 8 9 10 11 12 13 type AppendEntriesRequest struct { term // leader’s term leaderId // so follower can redirect clients prevLogIndex //index of log entry immediately preceding new ones prevLogTerm // term of prevLogIndex entry entries[] // log entries to store (empty for heartbeat; may send more than one for efficiency) leaderCommit // leader’s commitIndex } type AppendEntriesReply struct { term // currentTerm, for leader to update itself success // true if follower contained entry matching prevLogIndex and prevLogTerm } Reply false if term \u0026lt; currentTerm\nReply false if log doesn’t contain an entry at prevLogIndex whose term matches prevLogTerm\nIf an existing entry conflicts with a new one (same index but different terms), delete the existing entry and all that follow it\nAppend any new entries not already in the log\nIf leaderCommit \u0026gt; commitIndex, set commitIndex = min(leaderCommit, index of last new entry)\nRequsetVote\n1 2 3 4 5 6 7 8 9 10 11 type RequestVoteRequest struct { term // candidate’s term candidateId // candidate requesting vote lastLogIndex // index of candidate’s last log entry (§5.4) lastLogTerm // term of candidate’s last log entry (§5.4) } type RequsetVoteReply struct { term // currentTerm, for candidate to update itself voteGranted // true means candidate received vote } Reply false if term \u0026lt; currentTerm\nIf votedFor is null or candidateId, and candidate’s log is at least as up-to-date as receiver’s log, grant vote\nRules for Servers\nAll Servers:\nIf commitIndex \u0026gt; lastApplied: increment lastApplied, apply log[lastApplied] to state machine\nIf RPC request or response contains term T \u0026gt; currentTerm: set currentTerm = T, convert to follower\nFollowers:\nRespond to RPCs from candidates and leaders\nIf election timeout elapses without receiving AppendEntries RPC from current leader or granting the vote to the candidate: convert to candidate\nCandidates:\nOn conversion to candidate, start election:\nIncrement currentTerm\nVote for self\nReset election timer\nSend RequestVote RPCs to all other servers\nIf votes received from majority of servers: become leader\nIf AppendEntries RPC received from new leader: convert to follower\nIf election timeout elapses: start new election\nLeaders:\nUpon election: send initial empty AppendEntries RPCs (heartbeat) to each server; repeat during idle periods to prevent election timeouts\nIf command received from client: append entry to local log, respond after entry applied to state machine\nIf last log index ≥ nextIndex for a follower: send AppendEntries RPC with log entries starting at nextIndex\nIf successful: update nextIndex and matchIndex for follower\nIf AppendEntries fails because of log inconsistency: decrement nextIndex and retry\nIf there exists an N such that N \u0026gt; commitIndex, a majority of matchIndex[i] ≥ N, and log[N].term == currentTerm: set commitIndex = N.\n","date":"2023-08-31T00:00:00Z","image":"http://localhost:1313/p/raft-paper/4edbcd64-afa3-4eff-8d34-2624bf875149_hu_d1e36132be1300c7.webp","permalink":"http://localhost:1313/p/raft-paper/","title":"Raft Paper"},{"content":"Introduction\rIn lab 1 we will build a MapReduce system. Two main parts are implemented, one is a worker process that calls Map and Reduce functions and handles reading and writing files, and the other is a coordinator process that hands out tasks to workers and copes with failed workers.\nBefore you start the lab, must be familiar with Golang concurrency programming and RPC. Understanding the MapReduce paper is also essential.\nExperiment Description\rTo get started the lab 1, you should read the lab document.\nhttps://pdos.csail.mit.edu/6.824/labs/lab-mr.html\nBecause the RPC using based on Unix Socket, you must have a Linux System to Finish the experiment. (I choose the Ubuntu 22.04.2)\nA simple sequential MapReduce version in src/main/mrsequential.go. The code is helpful to understand to the whole system works.\nAccess to the system is main/mrcoordinator.go. When starting the program, it will call MakeCoordinator() to create a master. Secondly, the server() function will start Unix Socket Listen. Then, the main process will call Done() to check the total MapReduce whether finished. All of the above functions must be implemented in mr/coordinator.go.\nAs for the worker, we should put our implementation in mr/worker.go, and also refer the mrsequential.go.\nUsing the script test-mr.sh to check the program pass. Sometimes ctrl + c might not exis program, you should use ps to find the pid of the mrcoordinator process and kill it.\nImplementation\rExecution Overview\rThe master first assigns all map tasks to workers. In the map state, the worker will generate intermediate files \u0026quot;mr-X-Y\u0026quot;, where X is the Map task number, and Y is the Reduce task number. Then in the reduce state, the worker will input intermediate files, after being processed by reduce function, it will put the results into \u0026quot;mr-out-Y\u0026quot;.\nMaster Implementation\rNo lock is used, just ensured the data operation is thread-safe.\nFirst, the master will allocate tasks to workers, so we should definite task data structure. Filename handled by map function, Id is the worker num and it could be a map worker or a reduce worker. So we can merge the two tasks.\n1 2 3 4 type Task struct { Filename string Id int } Then, the master should record the state of the system. There is map, reduce, finish three states, just used int32 (0,1,2)to refer to them. Because we can\u0026rsquo;t use lock, make sure the work gets race task is used channel for synchronization. The MapTaskNum and ReduceTaskNum also needs to record the number of tasks assigned. Our system must tolerate machine failures gracefully. We should record the working time(Unix TimeStamp), a working time of more than 10 seconds is considered to be the failure of the machine. Each worker\u0026rsquo;s time has to be recorded and modified, and there are data races. To get good performance, I used sync.Map.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type Coordinator struct { State int32 // 0 map 1 reduce 2 finish MapTaskNum int ReduceTaskNum int MapTask chan Task ReduceTask chan Task TaskTime sync.Map files []string } const TimeOut = 10 type TimeStamp struct { Time int64 Fin bool } As the system begins, the master assigns map tasks to MapTask chan and record the working start time in TaskTime.\n1 2 3 4 5 for i, file := range files { now := time.Now().Unix() c.MapTask \u0026lt;- Task{FileName: file, Id: i} // send to chan c.TaskTime.Store(i, TimeStamp{now, false}) // record time } When a work sends a request to the master, it should get worker task, worker num, system state and a succeed mark to confirm that the task was acquired.\n1 2 3 4 5 6 7 8 type TaskRespond struct { WorkerTask Task MapTaskNum int ReduceTaskNum int State int32 Succeed bool } The master will send a reply to workers in different states.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func (c *Coordinator) GetTask(args *TaskRequest, reply *TaskRespond) error { state := atomic.LoadInt32(\u0026amp;c.State) if state == 0 { maptask, ok := \u0026lt;-c.MapTask if ok { reply.WorkerTask = maptask } else { return errors.New(\u0026#34;fail call task\u0026#34;) } } else if state == 1 { reducetask, rok := \u0026lt;-c.ReduceTask if rok { reply.WorkerTask = reducetask } else { return errors.New(\u0026#34;fail call task\u0026#34;) } } reply.MapTaskNum = c.MapTaskNum reply.ReduceTaskNum = c.ReduceTaskNum reply.State = c.State return nil } If a worker finish work, the master also will handle it in different states.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func (c *Coordinator) GetTaskFin(args *TaskRespond, _ *ExampleReply) error { state := args.State now := time.Now().Unix() id := args.WorkerTask.Id start, _ := c.TaskTime.Load(id) total := now - start.(TimeStamp).Time if total \u0026gt; TimeOut { return errors.New(\u0026#34;fail fin task\u0026#34;) } c.TaskTime.Store(id, TimeStamp{now, true}) if state == 0 { if lenTaskFin(\u0026amp;c.TaskTime) == c.MapTaskNum { close(c.MapTask) // close chan c.TaskTime = sync.Map{} for i := 0; i \u0026lt; c.ReduceTaskNum; i++ { c.ReduceTask \u0026lt;- Task{Id: i} c.TaskTime.Store(i, TimeStamp{now, false}) } atomic.StoreInt32(\u0026amp;c.State, 1) } } else if state == 1 { if lenTaskFin(\u0026amp;c.TaskTime) == c.ReduceTaskNum { atomic.StoreInt32(\u0026amp;c.State, 2) close(c.ReduceTask) // close chan } } return nil } At last, to tolerate machine failures we should assign its worker to another worker machine. In this system, I create a goroutine to check task finish periodically(2 seconds).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func (c *Coordinator) checkWorker() { for { time.Sleep(2 * time.Second) state := atomic.LoadInt32(\u0026amp;c.State) if empty(\u0026amp;c.TaskTime) { continue } if state == 0 { for i := 0; i \u0026lt; c.MapTaskNum; i++ { now := time.Now().Unix() tmp, _ := c.TaskTime.Load(i) if !tmp.(TimeStamp).Fin \u0026amp;\u0026amp; now-tmp.(TimeStamp).Time \u0026gt; TimeOut { fmt.Println(\u0026#34;map task timeout\u0026#34;) c.MapTask \u0026lt;- Task{FileName: c.files[i], Id: i} c.TaskTime.Store(i, TimeStamp{now, false}) } } } else if state == 1 { for i := 0; i \u0026lt; c.ReduceTaskNum; i++ { now := time.Now().Unix() tmp, _ := c.TaskTime.Load(i) if !tmp.(TimeStamp).Fin \u0026amp;\u0026amp; now-tmp.(TimeStamp).Time \u0026gt; TimeOut { fmt.Println(\u0026#34;reduce task timeout\u0026#34;) c.ReduceTask \u0026lt;- Task{Id: i} c.TaskTime.Store(i, TimeStamp{now, false}) } } } } } Worker Implementation\rThe worker loop fetches tasks from the master for execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { for { args := TaskRequest{} reply := TaskRespond{} CallGetTask(\u0026amp;args, \u0026amp;reply) if !reply.Succeed { goto sleep } if reply.State == 0 { doMapTask(\u0026amp;reply, mapf) } else if reply.State == 1 { doReduceTask(\u0026amp;reply, reducef) } else if reply.State == 2 { break } CallTaskFin(\u0026amp;reply) sleep: time.Sleep(1 * time.Second) } } The map and reduce work can refer to some code from mrsequential.go for reading Map input files, for sorting intermedate key/value pairs between the Map and Reduce, and for storing Reduce output in files.\nTo ensure that nobody observes partially written files in the presence of crashes, the MapReduce paper mentions the trick of using a temporary file and atomically renaming it once it is completely written. You can use ioutil.TempFile to create a temporary file and os.Rename to atomically rename it.\n","date":"2023-08-16T00:00:00Z","permalink":"http://localhost:1313/p/lab-1-mapreduce/","title":"Lab 1: MapReduce"},{"content":"Introduction\rMapReduce is a programming model mentioned by Google. It runs on a distributed system (like a cluster of commodity machines) and is highly scalable. In addition to the system, there is parallel computing of large-scale data sets.\nThis paper called MapReduce: Simplified Data Processing on Large Clusters was published in 2004 by Google engineers. Although is just thirteen pages and seems old nowadays, the information in the paper is also meaningful.\nProgramming Model\rMapReduce is a simple parallel model, in which the user specifies two functions: Map and Reduce\nMap: takes an input pair and produces a set of intermediate key/value pairs.\nReduce: accepts an intermediate key I and a set of values for that key. It merges together these values to form a possibly smaller set of values.\nConsider the problem of counting the number of occurrences of each word in large collections of documents:\n1 2 3 4 5 6 7 8 9 10 11 12 13 map(String key, String value): // key: document name // value: document content for each word w in value: EmitIntermediate(w, \u0026#34;1\u0026#34;) reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v) Emit(AsString(result)); Implementation\rThe issue of how to make distributed system may face many difficulties, such as parallelising the computation, distributing the data, and handling failures. Using the MapReduce model we can simplify it and implement it in a variety of ways.\nExecution Overview\rWhen the use program calls the MapReduce function, it will proceed as follows:\nThe program first split the input files into M pieces of typically 16 MB to 64 MB, then starts up many copies of the program.\nOne of the copies of the program is special - the master. The rest worker is assigned work by the master. There are M map and R reduce tasks to assign.\nA worker who is assigned a map task reads the contents of the corresponding input split. It parses key/value pairs out of the input data and passes each\npair to the user-defined Map function. The intermediate key/value pairs produced by the Map function are buffered in memory.\nPeriodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master, who is responsible for forwarding these locations to the reduce workers.\nWhen a reduce worker is notified by the master about these locations, it uses remote procedure calls to read the buffered data from the local disks of the map workers. When a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped\ntogether. The sorting is needed because typically many different keys map to the same reduce task. If the amount of intermediate data is too large to fit in\nmemory, an external sort is used.\nThe reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding\nset of intermediate values to the user’s Reduce function. The output of the Reduce function is appended to a final output file for this reduce partition.\nWhen all map tasks and reduce tasks have been completed, the master wakes up the user program. At this point, the MapReduce call in the user program returns back to the user code.\nAfter successful completion, the output of the MapReduce execution is available in the R output files. Typically, users do not need to combine these R output files into one file because they often pass these files as input to another MapReduce call.\nFault Tolerance\rThere is only a single master, its failure is unlikely. We just care about worker failure.\nThe master pings every worker periodically. If no response is received from a worker in a certain amount of time, the master marks the worker as failed. Any map tasks completed by the worker are reset back to their initial idle state, and therefore become eligible for scheduling on other workers. Similarly, any map task or reduce task in progress on a failed worker is also reset to idle and becomes eligible for rescheduling.\nCompleted map tasks are re-executed on a failure because their output is stored on the local disk(s) of the failed machine and is therefore inaccessible. Completed reduce tasks do not need to be re-executed since their output is stored in a global file system.\nWhen a map task is executed first by worker A and then later executed by worker B (because A failed), allworkers executing reduce tasks are notified of the re-execution. Any reduce task that has not already read the data from worker A will read the data from worker B.\nBackup Tasks\rSometimes there is a \u0026ldquo;straggler\u0026rdquo; lengthen the total time. For example, an aging disk on one machine causes read and write speeds to slow down. In this case, as the MapReduce operation is close to completion, the master schedules backup executions of the remaining in-progress tasks. The task is considered successful if either the original worker or the second assigned worker completes the task.\n","date":"2023-08-13T00:00:00Z","permalink":"http://localhost:1313/p/mapreduce-paper/","title":"MapReduce Paper"},{"content":"Introduction\rNot like CET-4 or CET-6, IELTS reading required efficiency. So you needn\u0026rsquo;t read the full text. Another feature is that the difficulty of IELTS reading is not increasing, which means you can\u0026rsquo;t finish the task in order. Look at the title first, then look at the question, and choose the easy one to finish.\nTRUE \u0026amp; FALSE, NOT GIVEN\rBefore starting the exercise, we should know two keys in IELTS reading: positing words and pointing words. (定位词和考点词)\npositioning words\rthe posting words can determine the location of the answer. So we should do the questions according to the test site, rather than guess.\nCommon positioning words\nNumbers (temperature, time, percentage) Capital letters (places, names, proper names)\nFind words occurrence in text only once (70%)\nDo not use digits to position in True or False and Heading\nSpecial names(90%) \u0026gt; special verbs(10%) \u0026gt; special adjectives or adverbs\nadjective closest to the noun\nadjective positional will occasionally use in Heading and Matching\nCommon words, topic words and repeated words in the question are not suitable\npointing words\rthe pointing words only appear in True or False, which is the examing point.\nClassification\nthe part after be, can\nDegree only, all, est\nCompare A B both words\nConsistency numbers, time, place\nTick the number first, then a noun, and read the text at the same time. If don\u0026rsquo;t find any pointing words, choose the verb and the information behind it.\nThe relationship between positing and pointing words\nthe positioning gives the answer location the pointing decides the answer options note\rTrue / Yes\nThe questions stem consistent with the text\nThe subject words are consistent with the test point words\nFalse / No\nThe question is contrary to the text\nthe subject words are consistent, but the points are contrary\nFalse classification\nThe \u0026ldquo;absolutely\u0026rdquo; statement may be wrong (80%) 绝对陈述词\nmust/absolutely/definitely/invariably/inevitably/unarguably/assuredly/indisputably/certainly/without question = out of the question\nimpossible/immediate/always/usually/never/all/every/any/none\nboth can use in Choice\nOn the contrary, the \u0026quot;possible\u0026quot; statement may be correct (70%)\nAntisense 反义\nIn the text, the positioning words around is the negative word.\nnot/no longer/not anymore/by not means(never)\nIn the text, the positioning words around the implying negation\nused to do sth/until recently/as was once the case\nSingle qualification 单一限定\nIn the text, \u0026quot;x and y are...\u0026quot;.\nIn the choice, \u0026quot;Only x is...\u0026quot; (False), \u0026quot;xxx is ...\u0026quot; (True)\nIn the text, \u0026quot;xxx is ...\u0026quot;\nIn the choice, \u0026quot;only xxx is ...\u0026quot; (NOT GIVEN)\nThe probability of selecting No among the numbers, but the age and time can\u0026rsquo;t be used. 题干有数字选错概率大\nNOT GIVEN\nNot mentioned 没提及\nPointing words not repeated 考点没出现\nNot have not a comparison 没有对比\nthan NG(70%)\nAuthority effect, public effect 将观点当成事实\nThe text is a person\u0026rsquo;s opinion, the topic stem is an objective statement of fact\nBest 最高级\ntext is \u0026ldquo;one of\u0026rdquo;, the choice is \u0026ldquo;est\u0026rdquo; (NG)\ntext is \u0026ldquo;est\u0026rdquo;, the choice is \u0026ldquo;one of\u0026rdquo; (True)\nSteps:\nCheck the question require 看题目要求\nFind Positing and pointing words 找定位词和考点词\nRead the text in groups of two questions, and look for the positioning and pointing points 每两个进行查找\nJudgment the answer 判断答案\nFill-in-the-Blank\rNo options summary\rorder 90% 不全是顺序题\nthe original words (90%) 不全是直接摘抄\nFind the positioning words near whitespace 找空格附近定位词\nSteps:\nReview the subject requirements 看题目要求（字数限制，看全文还是某一段）\nword count\nwhich paragraph(whole or part)\nRead the summary 通读summary\nFind positioning and relationship words 找定位词和关系词\nPredict the nature of the answer 预测答案性质\nsingular and plural for nouns\nVerb's tense\ncontent\nAdjust the answer according to the word count 调整答案性质\nA of B = B A = A (if A int text, write B)\nA by B = B\nPositioning words\nproper noun 专业名词\ntime 时间\nLocation 地点\nNames 名字\nFigures and percentages 数字和百分数\nMoney symbol 金钱\nSpecial print 特别印刷体\nVerb, preposition 动词或介词\nrelationship words\nmaintain or change the meaning between sentences, always indicate where the test site is.\nParalleling 并列\nand, as well as, or, too, also, neither...nor, first...second\nProgressive 递进\nmoreover, not only, but also, in addition\nCausality 因果\nbecause, as, since, in that + clause, due to, as a result of, because of\nso, therefore, thus, consequently, as a result.\nCiting 举例\nfor example, for instance, such as\nanalogy 类比\nlike similarly, be similar to, as...as\nadversative 转折\nbut, however, yet\nalthough, despite, while\nsentence completion\rskill like not options summary\noptions summary\runordered (30%) 乱序概率大 Steps:\nReview the topic 审题\nRead the options and organize them 看选项词汇并分类\nRead the summary 通读\nMark the position and relation words to locate the answer 标记定位词和关系词\nPositioning lookup versus options (30% out of order) 进行比对\ntable completion\rMost in consecutive paragraphs 出自连续的段落\nOrder 有序\nSimplest 最简单\nSteps\nRead the title 看标题\nFind logical relationships 找逻辑关系\nMark positioning words, relation words, predict the answer. 标记定位词，关系词，预测答案\nAdjust answer 调整答案\nWrite answer\rlike sentence completion 类似完成句子，但是是简答题\norder 有序\nSteps:\nStart with a clear line of questioning 明确提问的内容\nWhere/what place\nWhen/what time?\nWhat/which\nHow by/with/through\nPredicting the nature of the answer 预测答案\nLocate words and search 定位和查找\nAdjust answer 调整答案\nthe first problem may be difficult to locate 第一个问题可能比较难定位\nList of Heading\rThe question is before the text 题目在文章前面\nDifficult 困难\nNo Repeated choice 不会重复选\nSteps\nReading questions 读题干\nRead the example and remove it 读例子并去掉\nFilter the similar choice, which at least one of the antonyms to be true 过滤相似选项，至少一个是会被选\nExclude details such as points in time, money, percentages 排出一下细节\nPrediction 预测\nPrompts words of forward paragraph 首段提示词\nDefinition, concept, overview, introduction, initiation\nPrompts words of back paragraph 末段提示词\nConsequence, effect, conclusion, future, impact, prospects\nSpecial prompts words 特殊提示词\nfigure/number/amount/statistic(al)/data/demographics\nrate/ratio/proportion/percentage\ncost/fee/financial/business/income/salary/wage/revenue/deal\ntime/period\nCircle positing word 圈定位词\nreference True \u0026amp; False 参考判断题\nFind Purport 找主旨\nthe first sentence (70%) 首句\nthe second sentece (10%) 第二句\nthe last second sentence (10%) 末句\nunobvious (10%) 中间\nuse the repeated words 使用重复出现的词\nWrite answer 写答案\nlogical consistency, but note the synonymous substitutions 逻辑一致，注意同义替换\nUse the exclusion 使用排除法\nMatching\rName and option Matching\ruse the name as a choice or question 人名为选项或者题目\nunorder 无序\nname is the positing word 人名就是定位词\nsometimes a name with two options 有时人会对应多个观点\nSteps:\nSimplify names 简化人名\nRead the options and find keywords 看选项找关键词\nCompare options between choice and text 在原文和选项中比较\nbelieve, argue, suggest, claim, conclude, think\nAffiliation matching\runorder 无序 Steps:\nFind logical relationship 找逻辑关系\nCircle the positing words 圈定位词\nFind points and compare them with the text 找到观点和原文比较\nif the choice is shorter than the question, find the number of times a choice appears in the text\nWhich paragraph contains the following information\runorder 无序\nsmall skill 没有特点技巧\ncomplex synonymous substitution 复杂的替换\nI suggest this type of task at last. 建议最后写\nSteps:\nRead through the information and cricle the keywords 熟读并圈关键词\nIf don\u0026rsquo;t understand the question, give up on it. 如果题目看不懂，果断跳过\nFind positing words 找定位词\nRead the first and last sentences carefully 通常在第一句和最后一句\nIdentify the attribution 确定归属\nSingle \u0026amp; Multiple Choice\rorder 有序 Steps:\nDescription of the questions (quantity, directionality) 问题描述（数量，方向性）\nCheck the position words (both stem and choice) 找定位词\nLocate the text and find the points 定位原文找考点\nSummary\rThe order of tasks needs to be judged according to the type of question. Here are all the questions from hard to easy. (The ones in bold are sequential)\n三篇文章做题顺序需要根据题型来判断，阅读题目从难到易的排序\nWhich paragraph contains the following information\nList of Heading\nOptions Summary, Long Sentence Matching\nNames and Options, Affiliation matching\nNo Options Summary\nSingle Choice, Multiple Choice\nShort Sentence Matching\nSentence completion, write the answer\nTrue \u0026amp; False, Table Completion, notes\n","date":"2023-08-06T00:00:00Z","image":"http://localhost:1313/p/necessary-skills-of-ielts-reading/382f0a22-149c-4983-8de0-6495bea6fc88_hu_3b16c43ff0a8a923.jpeg","permalink":"http://localhost:1313/p/necessary-skills-of-ielts-reading/","title":"Necessary skills of IELTS (READING)"},{"content":"Introduction\rAs monolinguals, we face a lot of difficulties in IELTS.\nEnglish is not solid （基础差）\nNot familiar with IELTS （不懂考点）\nDon\u0026rsquo;t want to pay just want to escape（不想努力）\nTherefore, we can improve listening from the following there aspects.\nEnglish basis\rFirst, vocabulary is required. Then need to the spelling and pronunciation of each word.\n这里我使用墨墨背单词\n2 What\u0026rsquo;s on the IELTS listening? Difficulties in Listening\ntrap choice (陷阱选项)\nThe negation appears inaudible （听不到）\nDon\u0026rsquo;t understand the original text （不懂内容）\nThere is no detection of stealing the subject （没注意替换）\nIn general, this is considered as follows\nWords can be read and write Words can distinguish pronunciation and meaning Sentences can be read and translated Understand logical relationships Be familiar with substitutions and rephrasing Be clear about the test format Carefully prejudge the topic I think reading and writing words is the foundation, logical relationships and substitutions are key. (单词是基础，逻辑关系和替换是关键)\nHow to practice listening efficiency?\rFirst of all, there are four types of IELTS listening questions. They are fill-in-the-blank, map, matching, single-choice, and multiple-choice. There are tips and considerations for each type of question.\nFill-in-the-blank\rSteps:\nLook at the word limit （看题目单词限制） Circle the positioning words in the order of question number （划定位词） Circled words can be pronounced （知道被圈词的发音） Predict the content of the blank by circling （预测要填空的词） Write the blank considering whether it is plural or not （判断填空词的复数形式） the positioning word are usually the first and after two\nMore stable: number, noun, proper noun\nUnstable: adjective, verb\nTrap: adjective, common antonyms, negative words\nMap\rSteps:\nMark directions (such as top, bottom, left and right, or east, west, north and south) （标记方位） Circle all the place names and know the pronunciation （圈出地名并知道发音） Mark the picture in the order of question number （按顺序标记问题） If the topic stem is in lowercase, it may not be directly addressable （如果题目是小写，可能发送改写） Pay attention to the logical relation link words and answer （注意逻辑关系词） Sometimes there will be a fill-in-the-blank-map, the method is also applicable.\nMatching\rSteps:\nFind the positioning words in the choices (not repeated and non-specific words) （找定位词） Pay attention to logical relationships （注意逻辑关系） However, we should be paid to logical relation words and synonymous substitutions\nLogical relation\nParalleling\nand, also, as well as, then, another, anything else? is that all?\nComparison\nat the same time\nOrder\nfirstly, then, after that, lastly, and finally\nExplanation\nI mean, this means, in other words, for instance\nExamples\nsuch as, like, for example\nTurn\nbut, however, all of a sudden, whereas\nCause and Effect\nbecause, since, so, therefore\nEmphasis\nin fact, actually, especially, do + (verb), of course, important\nParenthesis\nlet me see, you know...\nNew Topic\nnow, anyway\nSupplement\nOh, hang on, wait a minute\nSynonymous substitutions\nShort rephrased\nChange the property of the word\nreduce = reduction\nSynonyms\nreduce = fail = dive - go down = decrease = decline\nSimilar words\nassignment = presentation = essay = reflect (paper)\nhash brown = chips = french fries = potato (food)\nExplain\nbuying things = shop\nDouble negatives\nnot good = bad\nPronoun substitution\nJames = he\nContext\nfirst = succeed\nInvert\nTom's books = the books of Tom\npriority for revision = revision priority\nLong rephrased\nA = B\nPassive = Active\nsingle-choice \u0026amp; multiple-choice\rSteps:\nRead and mark the question （优先看完问题，审题）\nUnderline test points, pay attention to substitution, Know the pronunciation （考点划线，注意改写，生词猜发音）\nMake notes when listening （边听边做记号）\nTips\nThree major positioning methods\nDon\u0026rsquo;t understand + choice original word = trap\nIt\u0026rsquo;s safer to hear rewrites\nPay attention to the juxtaposition of options\nPay attention to the identity and gender of the person speaking\nFocus on positioning\nThe original word in the question\nLogical connectives\nRephrase the question\nSummary\rTry to look at all the choices （题目选项尽量都看）\nLook and draw a circle （边看边画圈预判）\nLook at each section in order of question number （每个部分按题号顺序看）\nNotice the timing （注意时间）\nThe third part is most difficult to recommend squeezing in time to see in advance （第三部分最难建议挤时间提前看）\n","date":"2023-08-04T00:00:00Z","image":"http://localhost:1313/p/necessary-skills-of-ielts-listening/382f0a22-149c-4983-8de0-6495bea6fc88_hu_3b16c43ff0a8a923.jpeg","permalink":"http://localhost:1313/p/necessary-skills-of-ielts-listening/","title":"Necessary skills of IELTS (LISTENING)"},{"content":"前言\r第二章为Program Structure（程序结构），通过7个部分的示例代码深入讨论Go程序基础结构方面的一些细节。第二章课后练习比较基础，了解Go语言程序结构即可完成。\n本章所有示例代码链接 ch2 (有需自取，这里不做演示)\n课后练习\rPackages and Files\r示例代码是一个进行摄氏度和华氏度转换的程序，课后练习如下\n练习 2.1： 向tempconv包添加类型、常量和函数用来处理Kelvin绝对温度的转换，Kelvin 绝对零度是−273.15°C，Kelvin绝对温度1K和摄氏度1°C的单位间隔是一样的。\n思路\n类似摄氏度和华氏度之间的转换，将开尔文与其转换。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // tempconv.go in the tempconv package type Celsius float64 type Fahrenheit float64 type Kelvin float64 const ( AbsoluteZeroC Celsius = -273.15 freezingF Celsius = 0 boilingC Celsius = 100 ) func (c Celsius) String() string { return fmt.Sprintf(\u0026#34;%g°C\u0026#34;, c) } func (f Fahrenheit) String() string { return fmt.Sprintf(\u0026#34;%g°F\u0026#34;, f) } func (k Kelvin) String() string { return fmt.Sprintf(\u0026#34;%g°K\u0026#34;, k) } func CToF(c Celsius) Fahrenheit { return Fahrenheit(c*9/5 + 32) } func FToC(f Fahrenheit) Celsius { return Celsius((f - 32) * 5 / 9) } func KToC(k Kelvin) Celsius { return Celsius(k) - AbsoluteZeroC } func CToK(c Celsius) Kelvin { return Kelvin(c + AbsoluteZeroC) } func FToK(f Fahrenheit) Kelvin { return kelvin(KToC(f) + AbsoluteZeroC) } func KToF(k Kelvin) Fahrenheit { return CTok(Celsius(k) - AbsoluteZeroC) } Imports\r示例代码是一个调用tempconv包中转换函数的程序，课后练习如下\n练习 2.2： 写一个通用的单位转换程序，用类似cf程序的方式从命令行读取参数，如果缺省的话则是从标准输入读取参数，然后做类似Celsius和Fahrenheit的单位转换，长度单位可以对应英尺和米，重量单位可以对应磅和公斤等。\n思路\n这里以英尺和米的转换为例\n1 2 3 4 5 6 7 8 9 10 11 12 13 type Meter float64 type Foot float64 func (m Meter) String() string { return fmt.Sprintf(\u0026#34;%fm\u0026#34;, m) } func (f Foot) String() string { return fmt.Sprintf(\u0026#34;%fft\u0026#34;, f) } func (m Meter) MToF() Foot { return Foot(m / 0.3084) } func (f Foot) FToM() Meter { return Meter(f * 0.3084) } Package Initialization\r示例代码是一个统计二进制数学中bit为1个数的程序，其中使用init方法初始化程序，课后练习如下。\n练习 2.3： 重写PopCount函数，用一个循环代替单一的表达式。比较两个版本的性能。（11.4节将展示如何系统地比较两个不同实现的性能。）\n练习 2.4： 用移位算法重写PopCount函数，每次测试最右边的1bit，然后统计总数。比较和查表算法的性能差异。\n练习 2.5： 表达式x\u0026amp;(x-1)用于将x的最低的一个非零的bit位清零。使用这个算法重写PopCount函数，然后比较性能。\n思路\n用for循环代替原函数的暴力相加即可。使用移位算法，通过x\u0026amp;1计算最低位是否为1，然后执行x\u0026gt;\u0026gt;1移位，依次统计为1的次数即可。使用表达式x\u0026amp;(x-1)可以将x的最低的一个非零bit位清零，因此只需统计执行次数即可。至于性能判断，通过time包下的Now()和Since()方法比较不同函数执行时间即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 func PopCount2(x uint64) int { var res int for i := 1; i \u0026lt; 8; i++ { res += int(pc[byte(x\u0026gt;\u0026gt;(i*8))]) } return res } // count each bit func PopCount3(x uint64) int { var res int for x \u0026gt; 0 { if x\u0026amp;1 == 1 { res++ } x \u0026gt;\u0026gt;= 1 } return res } // x \u0026amp; (x - 1) func PopCount4(x uint64) int { var res int for x \u0026gt; 0 { x \u0026amp;= (x - 1) res++ } return res } ","date":"2023-07-31T00:00:00Z","image":"http://localhost:1313/p/the-go-programming-language-%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0-ch.2/ab638d52-5f92-4e29-955e-064deb1c4728_hu_42190784e75fa0e4.jpeg","permalink":"http://localhost:1313/p/the-go-programming-language-%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0-ch.2/","title":"The Go Programming Language 课后练习 Ch.2"},{"content":"前言\r第一章为Tutorial（教程），通过8个部分的示例代码带领读者熟悉Go语言的语法和规则。第一章课后练习比较基础，熟悉基本语法规则即可完成。\n本章所有示例代码链接 Ch1 (有需求自取，就不给出示例代码了。。。)\n课后练习\rCommand-Line Arguments\r示例代码是一个打印命令行参数的程序，课后练习如下\n练习 1.1： 修改 echo 程序，使其能够打印 os.Args[0]，即被执行命令本身的名字\n练习 1.2： 修改 echo 程序，使其打印每个参数的索引和值，每个一行。\n练习 1.3： 做实验测量潜在低效的版本和使用了strings.Join的版本的运行时间差异。（1.6节讲解了部分time包，11.4节展示了如何写标准测试程序，以得到系统性的性能评测。）\n思路\n这三个练习还是非常基础的，利用for range遍历os.Args[0:]切片即可，range会返回元素和该元素对应的下标（从0开始）。示例代码中使用字符串拼接的方式来获取命令行参数结果，但其实这是一种效率非常低的方法，因此另一个示例代码用stirngs.Join方法拼接字符串，这是一种更高效的方法（原因参考原文）。为了测试两种方式的效率，使用time包中的Now()和Since()方法即可获得程序运行时间，比较运行时间即可测评性能。\n1 2 3 4 5 func main() { for i, arg := range os.Args[0:] { fmt.Printf(\u0026#34;index = %d, value = %s\\n\u0026#34;, i, arg) } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func main() { start := time.Now() s, sep := \u0026#34;\u0026#34;, \u0026#34;\u0026#34; for _, arg := range os.Args[0:] { s += sep + arg sep = \u0026#34; \u0026#34; } fmt.Println(s) // strings.Join // fmt.Println(strings.Join(os.Args[0:], \u0026#34; \u0026#34;)) total := time.Since(start) fmt.Printf(\u0026#34;runing time = %s\\n\u0026#34;, total) } Finding Duplicate Lines\r示例代码是一个读取标准输入流或者命令行参数中的文件, 并找出重复行的程序。课后练习如下\n练习 1.4： 修改 dup2，出现重复的行时打印文件名称。\n思路\n原代码通过维护一个map来记录每行的内容和出现次数，因此需维护另一个map来记录每行内容和出现该内容的file数组。最后在输出重复行时，将file数组一同输出即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 // if target in strs func contains(strs []string, target string) bool { for _, str := range strs { if str == target { return true } return false } // find duplicate line func countLines(f *os.File, counts map[string]int, filename map[string][]string) { intput := bufio.NewScanner(f) for input.Scan() { str := input.Text() counts[str]++ if filename == nil { filename[str] = make([]string, 0) } if !contains(filename[str], f.Name()) { filename[str] = append(filename[str], f.Name()) } } func main() { counts := make(map[string]int) filename := make(map[string][]string) files := os.Args[1:] if len(files) == 0 { countLines(os.Stdin, counts, filename) } else { for _, arg := range files { f, err := os.Open(arg) if err != nil { fmt.Fprintf(os.Stderr, \u0026#34;dup2: %v\\n\u0026#34;, err) continue } // dup line countLines(f, counts, filename) f.Close() } } // output info for line, n := range counts { if n \u0026gt; 1 { fmt.Printf(\u0026#34;%d\\t%s\\t%v\\n\u0026#34;, n, line, filename[line]) } } } Animated GIFs\r示例代码是一个利用sin函数和cos函数打印利萨如图形（Lissajous figures）的程序。课后练习如下\n练习 1.5： 修改前面的Lissajous程序里的调色板，由黑色改为绿色。我们可以用color.RGBA{0xRR, 0xGG, 0xBB, 0xff}来得到#RRGGBB这个色值，三个十六进制的字符串分别代表红、绿、蓝像素。\n练习 1.6： 修改Lissajous程序，修改其调色板来生成更丰富的颜色，然后修改SetColorIndex的第三个参数，看看显示结果吧。\n思路\npalette数组为图形调色板，SetColorIndex函数第三个参数对应调色板颜色的下标。将七彩色放入调色板，每次运行后改变颜色即可得到RGBGIF。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 var palette = []color.Color{ //color.Black, color.White, color.RGBA{0xff, 0x00, 0x00, 0xff}, color.RGBA{0xff, 0x7d, 0x00, 0xff}, color.RGBA{0xff, 0xff, 0x00, 0xff}, color.RGBA{0x00, 0xff, 0x00, 0xff}, color.RGBA{0x00, 0x00, 0xff, 0xff}, color.RGBA{0x00, 0xff, 0xff, 0xff}, color.RGBA{0xff, 0x00, 0xff, 0xff}, } func lissajous(out io.Writer) { const ( cycles = 5 res = 0.001 size = 100 nframes = 64 delay = 8 ) freq := rand.Float64() * 3.0 anim := gif.GIF{LoopCount: nframes} phase := 0.0 colorIndex := 1 for i := 0; i \u0026lt; nframes; i++ { rect := image.Rect(0, 0, 2*size+1, 2*size+1) img := image.NewPaletted(rect, palette) for t := 0.0; t \u0026lt; cycles*2*math.Pi; t += res { x := math.Sin(t) y := math.Sin(t*freq + phase) img.SetColorIndex(size+int(x*size+0.5), size+int(y*size+0.5), uint8(colorIndex)) } colorIndex = (colorIndex % 7) + 1 phase += 0.1 anim.Delay = append(anim.Delay, delay) anim.Image = append(anim.Image, img) } gif.EncodeAll(out, \u0026amp;anim) } Fetching a URL\r示例代码是一个获取对应的url，发送http请求，并将其源文本打印出来的程序，课后练习如下\n练习 1.7： 函数调用io.Copy(dst, src)会从src中读取内容，并将读到的结果写入到dst中，使用这个函数替代掉例子中的ioutil.ReadAll来拷贝响应结构体到os.Stdout，避免申请一个缓冲区（例子中的b）来存储。记得处理io.Copy返回结果中的错误。\n练习 1.8： 修改fetch这个范例，如果输入的url参数没有 http:// 前缀的话，为这个url加上该前缀。你可能会用到strings.HasPrefix这个函数。\n练习 1.9： 修改fetch打印出HTTP协议的状态码，可以从resp.Status变量得到该状态码。\n思路\n这三个练习较为基础，用io.Copy代替ioutill.ReadAll，将返回信息输出到stdout中。通过strings.HasPrefix来判断url是否有http://前缀，没有就加上。最后打印resp.Status即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func main() { for _, url := range os.Args[1:] { if !strings.HasPrefix(url, \u0026#34;http://\u0026#34;) { url = \u0026#34;http://\u0026#34; + url } resp, err := http.Get(url) if err != nil { fmt.Fprint(os.Stderr, \u0026#34;fetch: %v\\n\u0026#34;, err) os.Exit(1) } //b, err := ioutil.ReadAll(resp.Body) fmt.Printf(\u0026#34;http status: %s\\n\u0026#34;, resp.Status) b, err := io.Copy(os.Stdout, resp.Body) resp.Body.Close() if err != nil { fmt.Fprint(os.Stderr, \u0026#34;fetch: iocopy %s: %v\\n\u0026#34;, url, err) os.Exit(1) } fmt.Printf(\u0026#34;%d\\n\u0026#34;, b) } } Fetching URLS Concurrently\r示例代码和上一小节程序所要做的工作基本一致，但fetchall的特别之处在于它会同时去获取所有的URL，所以这个程序的总执行时间不会超过执行时间最长的那一个任务，前面程序执行时间则是所有任务执行时间之和。课后练习如下\n练习 1.10： 找一个数据量比较大的网站，用本小节中的程序调研网站的缓存策略，对每个URL执行两遍请求，查看两次时间是否有较大的差别，并且每次获取到的响应内容是否一致，修改本节中的程序，将响应结果输出，以便于进行对比。\n练习 1.11： 在fetchall中尝试使用长一些的参数列表，比如使用在alexa.com的上百万网站里排名靠前的。如果一个网站没有回应，程序将采取怎样的行为？（Section8.9 描述了在这种情况下的应对机制）。\n思路\n这里涉及的内容为计算机网络内容，个人无法给出规范答案，因此由ChatGPT作答\n1.10\n根据网站的缓存策略，对每个URL执行两遍请求，两次执行时间可能会有较大的差别，这取决于缓存的设置和实际的网络环境。当涉及到缓存的情况时，以下两种情况是可能的：\n第一次请求：如果该URL的响应之前没有被缓存，服务器将会生成响应并将其返回给客户端。在这种情况下，第一次请求可能会比较慢，因为服务器需要处理请求并生成响应。\n第二次请求：如果该URL的响应被缓存了，服务器会直接返回缓存的响应，而无需再次处理请求。在这种情况下，第二次请求通常会比较快，因为省去了服务器重新生成响应的步骤。\n因此，第一次请求通常比第二次请求慢，但具体的差别取决于缓存策略、缓存的生存时间以及服务器和网络的负载等因素。\n关于响应内容的一致性，如果两次请求之间没有发生数据变化，并且没有其他中间因素影响响应内容，那么两次获取到的响应内容应该是一致的。这是因为缓存是根据URL或其他标识来存储响应的，如果请求的标识相同，那么缓存会返回相同的响应内容。 然而，需要注意以下几点：\n如果两次请求之间的数据发生了变化（例如，某些数据被更新了），则两次响应的内容可能会不一致，因为第二次请求获取到的是最新的数据。\n如果网站的缓存策略是根据时间来进行刷新，那么两次请求之间的时间间隔较长，第二次请求可能会从服务器上获取到更新后的响应内容。\n在多服务器或负载均衡环境中，两次请求可能被分发到不同的服务器上，导致两次响应内容不一致，这取决于服务器之间是否共享缓存。\n综上所述，根据网站的缓存策略，两次请求的执行时间可能有较大的差别，并且每次获取到的响应内容在一些特定情况下可能会不一致。但在正常情况下，如果两次请求之间没有发生变化并且没有其他中间因素干扰，响应内容应该是一致的。\n1.11\n在fetchall代码中，如果尝试使用长一些的参数列表，例如使用在 Alexa 排名靠前的上百万网站作为URL列表，当其中某个网站没有回应时，程序的行为将根据并发请求的方式来决定。\n根据目前给出的代码，fetchall是使用一个简单的循环来依次请求每个URL的，因此程序会按照顺序一个一个地发送请求。在这种情况下，如果一个网站没有回应，程序会出现阻塞等待的情况，直到该请求超时或获得响应。\n具体表现如下：\n阻塞等待：当程序发起请求并没有获得响应时，程序会一直等待，直到超过了设置的请求超时时间。这可能会导致程序执行时间较长，尤其在长列表中有多个未响应的网站时。\n超时报错：如果请求超时，http.Get方法会返回一个错误，指示请求超时。在代码中，该错误会被捕获，并在标准错误输出中打印出来。随后，程序继续执行下一个请求。\n值得注意的是，超时时间的设置在当前给出的代码中并未体现。对于大规模请求或长列表，设置合理的超时时间是非常重要的。如果没有设置超时时间，程序在遇到未响应的请求时会一直等待，可能会导致整个程序执行时间过长或出现假死状态。\n为了优化fetchall的行为，可以考虑采用并发的方式发送请求，从而避免阻塞等待问题。可以使用 Goroutines 和通道来实现并发请求，从而提高程序的效率。在并发的情况下，当某个请求超时或出现问题时，其他请求仍可以继续执行，不会受到阻塞。这样可以更好地利用系统资源，并加快整体请求的处理速度。\n以下是一个简单的示例代码，展示了如何使用 Goroutines 和通道来并发地发送请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) func fetch(url string, ch chan\u0026lt;- string) { start := time.Now() resp, err := http.Get(url) if err != nil { ch \u0026lt;- fmt.Sprintf(\u0026#34;fetch error: %s %v\u0026#34;, url, err) return } defer resp.Body.Close() b, err := ioutil.ReadAll(resp.Body) if err != nil { ch \u0026lt;- fmt.Sprintf(\u0026#34;read error: %s %v\u0026#34;, url, err) return } ch \u0026lt;- fmt.Sprintf(\u0026#34;%.2fs %s\u0026#34;, time.Since(start).Seconds(), b) } func main() { start := time.Now() ch := make(chan string) for _, url := range os.Args[1:] { go fetch(url, ch) } for range os.Args[1:] { fmt.Println(\u0026lt;-ch) } fmt.Printf(\u0026#34;Total time: %.2fs\\n\u0026#34;, time.Since(start).Seconds()) } 在这个示例中，fetch函数被并发地调用，并使用通道ch来收集每个请求的结果。当一个请求完成时，它会将结果发送到通道中，然后主函数从通道中接收结果并打印。这样，即使其中某个请求没有回应，其他请求仍然可以继续执行，不会出现阻塞等待的情况。\nA WebServer\r示例代码为一个简单的Web服务器程序。由于go标准库实现了大量的工作，我们只需写少数代码即可实现服务器。课后练习如下\n练习 1.12： 修改Lissajour服务，从URL读取变量，比如你可以访问 http://localhost:8000/?cycles=20 这个URL，这样访问可以将程序里的cycles默认的5修改为20。字符串转换为数字可以调用strconv.Atoi函数。你可以在godoc里查看strconv.Atoi的详细说明。\n思路\n解析url中的cycles参数，然后改变lissajour函数，将cycles参数传入其中，就可以变化gif图形\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func main() { handler := func(w http.ResponseWriter, r *http.Request) { var cycles string for k, v := range r.Form { if k == \u0026#34;cycles\u0026#34; { cycles = v[0] } } cycles_int, err := strconv.Atoi(cycles) if err != nil { cycles_int = 5 } lissajous(w, cycles_int) } http.HandleFunc(\u0026#34;/\u0026#34;, handler) log.Fatal(http.ListenAndServe(\u0026#34;localhost:8000\u0026#34;, nil)) } ","date":"2023-07-27T00:00:00Z","image":"http://localhost:1313/p/the-go-programming-language-%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0-ch.1/ab638d52-5f92-4e29-955e-064deb1c4728_hu_42190784e75fa0e4.jpeg","permalink":"http://localhost:1313/p/the-go-programming-language-%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0-ch.1/","title":"The Go Programming Language 课后练习 Ch.1"}]